{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2e00e48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Available methods are the followings:\n",
    "[1] BetaCalibration\n",
    "[2] ABM_BetaCal\n",
    "[3] AB_BetaCal\n",
    "[4] AM_BetaCal\n",
    "[5] Sigmoid_Cal\n",
    "\n",
    "Authors: Danusorn Sitdhirasdr <danusorn.si@gmail.com>\n",
    "versionadded:: 20-09-2025\n",
    "\n",
    "'''\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from sklearn.metrics import brier_score_loss, log_loss, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import column_or_1d, check_consistent_length\n",
    "from sklearn.utils.validation import check_array\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from scipy.optimize import minimize_scalar\n",
    "from sklearn.utils.validation import check_is_fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24d41e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = [\"BetaCalibration\",\n",
    "           \"ABM_BetaCal\",\n",
    "           \"AB_BetaCal\",\n",
    "           \"AM_BetaCal\", \n",
    "           \"Sigmoid_Cal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a557bbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidateParams:\n",
    "    \n",
    "    '''Validate parameters'''\n",
    "    \n",
    "    def Interval(self, Param, Value, dtype=int, \n",
    "                 left=None, right=None, closed=\"both\"):\n",
    "\n",
    "        '''\n",
    "        Validate numerical input.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Param : str\n",
    "            Parameter's name\n",
    "\n",
    "        Value : float or int\n",
    "            Parameter's value\n",
    "\n",
    "        dtype : {int, float}, default=int\n",
    "            The type of input.\n",
    "\n",
    "        left : float or int or None, default=None\n",
    "            The left bound of the interval. None means left bound is -∞.\n",
    "\n",
    "        right : float, int or None, default=None\n",
    "            The right bound of the interval. None means right bound is +∞.\n",
    "\n",
    "        closed : {\"left\", \"right\", \"both\", \"neither\"}\n",
    "            Whether the interval is open or closed. Possible choices are:\n",
    "            - \"left\": the interval is closed on the left and open on the \n",
    "              right. It is equivalent to the interval [ left, right ).\n",
    "            - \"right\": the interval is closed on the right and open on the \n",
    "              left. It is equivalent to the interval ( left, right ].\n",
    "            - \"both\": the interval is closed.\n",
    "              It is equivalent to the interval [ left, right ].\n",
    "            - \"neither\": the interval is open.\n",
    "              It is equivalent to the interval ( left, right ).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Value : float or int\n",
    "            Parameter's value\n",
    "\n",
    "        '''\n",
    "        Options = {\"left\"    : (np.greater_equal, np.less), # a<=x<b\n",
    "                   \"right\"   : (np.greater, np.less_equal), # a<x<=b\n",
    "                   \"both\"    : (np.greater_equal, np.less_equal), # a<=x<=b\n",
    "                   \"neither\" : (np.greater, np.less)} # a<x<b\n",
    "\n",
    "        f0, f1 = Options[closed]\n",
    "        c0 = \"[\" if f0.__name__.find(\"eq\")>-1 else \"(\" \n",
    "        c1 = \"]\" if f1.__name__.find(\"eq\")>-1 else \")\"\n",
    "        v0 = \"-∞\" if left is None else str(dtype(left))\n",
    "        v1 = \"+∞\" if right is None else str(dtype(right))\n",
    "        if left  is None: left  = -np.inf\n",
    "        if right is None: right = +np.inf\n",
    "        interval = \", \".join([c0+v0, v1+c1])\n",
    "        tuples = (Param, dtype.__name__, interval, Value)\n",
    "        err_msg = \"%s must be %s or in %s, got %s \" % tuples    \n",
    "\n",
    "        if isinstance(Value, dtype):\n",
    "            if not (f0(Value, left) & f1(Value, right)):\n",
    "                raise ValueError(err_msg)\n",
    "        else: raise ValueError(err_msg)\n",
    "        return Value\n",
    "\n",
    "    def StrOptions(self, Param, Value, options, dtype=str):\n",
    "\n",
    "        '''\n",
    "        Validate string or boolean inputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Param : str\n",
    "            Parameter's name\n",
    "            \n",
    "        Value : float or int\n",
    "            Parameter's value\n",
    "\n",
    "        options : set of str\n",
    "            The set of valid strings.\n",
    "\n",
    "        dtype : {str, bool}, default=str\n",
    "            The type of input.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Value : float or int\n",
    "            Parameter's value\n",
    "\n",
    "        '''\n",
    "        if Value not in options:\n",
    "            err_msg = f'{Param} ({dtype.__name__}) must be either '\n",
    "            for n,s in enumerate(options):\n",
    "                if n<len(options)-1: err_msg += f'\"{s}\", '\n",
    "                else: err_msg += f' or \"{s}\" , got %s'\n",
    "            raise ValueError(err_msg % Value)\n",
    "        return Value\n",
    "    \n",
    "    def check_range(self, param0, param1):\n",
    "        \n",
    "        '''\n",
    "        Validate number range.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        param0 : tuple(str, float)\n",
    "            A lower bound parameter e.g. (\"name\", -100.)\n",
    "            \n",
    "        param1 : tuple(str, float)\n",
    "            An upper bound parameter e.g. (\"name\", 100.)\n",
    "            \n",
    "        '''\n",
    "        if param0[1] >= param1[1]:\n",
    "            raise ValueError(f\"`{param0[0]}` ({param0[1]}) must be less\"\n",
    "                             f\" than `{param1[0]}` ({param1[1]}).\")\n",
    "            \n",
    "    def check_y_inputs(self, y_proba, y_true=None):\n",
    "        \n",
    "        '''\n",
    "        Validate inputs for calibration models.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_proba : array-like, shape (n_samples,)\n",
    "            Predicted probabilities ranges from 0 to 1.\n",
    "            \n",
    "        y_true : array-like, shape (n_samples,), default=None\n",
    "            True binary labels (0/1).\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y_proba : ndarray shape (n_samples,)\n",
    "            Validated probabilities.\n",
    "            \n",
    "        y_true : ndarray shape (n_samples,)\n",
    "            Validated labels. This returns None when `y_ture` is not \n",
    "            provided.\n",
    "            \n",
    "        '''\n",
    "        # Ensure shapes\n",
    "        y_proba = column_or_1d(y_proba)\n",
    "        y_proba = check_array(y_proba, ensure_2d=False, dtype=float)\n",
    "\n",
    "        # Probability range\n",
    "        eps = np.finfo(float).eps\n",
    "        y_proba = np.clip(y_proba, eps, 1 - eps)\n",
    "        \n",
    "        if y_true is not None:\n",
    "            \n",
    "            # Check lengths\n",
    "            y_true = column_or_1d(y_true)\n",
    "            check_consistent_length(y_proba, y_true)\n",
    "\n",
    "            # Check that y_true only contains {0,1}\n",
    "            unique_labels = np.unique(y_true)\n",
    "            if not np.all(np.isin(unique_labels, [0, 1])):\n",
    "                raise ValueError(\"`y_true` must be binary (0 or 1).\")\n",
    "            return y_proba, y_true\n",
    "        \n",
    "        else: return y_proba, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68ea8603",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateLoss:\n",
    "    \n",
    "    '''\n",
    "    Utility class to evaluate model performance before and after \n",
    "    calibration.\n",
    "\n",
    "    Metrics:\n",
    "        - Brier score\n",
    "        - Log loss\n",
    "        - Gini coefficient\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    @staticmethod\n",
    "    def _gini(y_true, y_proba):\n",
    "        \n",
    "        '''\n",
    "        Compute Gini coefficient from ROC AUC.\n",
    "        '''\n",
    "        return 2 * roc_auc_score(y_true, y_proba) - 1\n",
    "\n",
    "    def evaluate(self, y_true, y_proba, y_calib):\n",
    "        \n",
    "        '''\n",
    "        Evaluate loss metrics before and after calibration.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : array-like of shape (n_samples,)\n",
    "            Ground truth binary labels.\n",
    "\n",
    "        y_proba : array-like of shape (n_samples,)\n",
    "            Predicted probabilities before calibration.\n",
    "\n",
    "        y_calib : array-like of shape (n_samples,)\n",
    "            Predicted probabilities after calibration.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        losses : namedtuple\n",
    "            Pre and post calibration losses (Brier score and log-loss)\n",
    "            \n",
    "        '''\n",
    "        a, b = (y_true, y_proba), (y_true, y_calib)\n",
    "        losses = {\"brier_score\": [float(brier_score_loss(*a)), \n",
    "                                  float(brier_score_loss(*b))],\n",
    "                  \"log_loss\"   : [float(log_loss(*a)), \n",
    "                                  float(log_loss(*b))]}\n",
    "        \n",
    "        return namedtuple(\"Losses\", losses.keys())(**losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0082cb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABM_BetaCal(BaseEstimator, RegressorMixin, ValidateParams, EvaluateLoss):\n",
    "    \n",
    "    '''\n",
    "    Beta regression model with three parameters introduced in \n",
    "    Kull, M., Silva Filho, T.M. and Flach, P. (2017).\n",
    "    Beta calibration: a well-founded and easily implemented \n",
    "    improvement on logistic calibration for binary classifiers.\n",
    "    AISTATS 2017.\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    This implementation is adapted from the `betacal` Python library \n",
    "    (https://github.com/dirmeier/betacal) with modifications.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    params_ : namedtuple\n",
    "        Model coefficients: a, b, c (intercept), and m (midpoint).\n",
    "        \n",
    "    calibrator_ : sklearn.linear_model.LogisticRegression\n",
    "        Internal logistic regression used to train the model.\n",
    "        \n",
    "    losses_ : namedtuple\n",
    "        Pre and post calibration losses (Brier score and log-loss).\n",
    "\n",
    "    '''\n",
    "    \n",
    "    def __transform__(self, y_proba):\n",
    "        \n",
    "        '''\n",
    "        Transform probabilities into 2D-array for logistic regression.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y_proba : array-like, shape (n_samples,)\n",
    "            Probability estimates.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        X : array-like, shape (n_samples, 2)\n",
    "            Transformed X.\n",
    "        \n",
    "        '''\n",
    "        eps = np.finfo(float).eps\n",
    "        X = np.clip(y_proba, eps, 1 - eps).reshape(-1,1)\n",
    "        X = np.log(np.hstack((X, 1.0 - X)))\n",
    "        X[:, 1] *= -1\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def fit(self, y_proba, y_true, sample_weight=None):\n",
    "        \n",
    "        '''\n",
    "        Fit the model using y_proba, y_true as training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_proba : array-like, shape (n_samples,)\n",
    "            Probability estimates.\n",
    "            \n",
    "        y_true : array-like, shape (n_samples,)\n",
    "            Training data.\n",
    "\n",
    "        sample_weight : array-like, shape = (n_samples,), default=None \n",
    "            Sample weights. If None, then samples are equally weighted.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of self.\n",
    "            \n",
    "        '''\n",
    "        # Validate inputs\n",
    "        y_proba, y = self.check_y_inputs(y_proba, y_true)\n",
    "        X = self.__transform__(y_proba)\n",
    "     \n",
    "        # Initial model fitting\n",
    "        kwargs = dict(C=1e12, penalty='l2', solver='lbfgs', fit_intercept=True)\n",
    "        calibrator = LogisticRegression(**kwargs).fit(X, y, sample_weight)\n",
    "        a, b = calibrator.coef_.ravel()\n",
    "        self.index = [0,1]\n",
    "        \n",
    "        # Adjust if coefficients are negative\n",
    "        if a < 0:\n",
    "            self.index = [1]\n",
    "            calibrator.fit(X[:,self.index], y, sample_weight)\n",
    "            a, b = 0, calibrator.coef_.ravel()[0]\n",
    "        elif b < 0:\n",
    "            self.index = [0]\n",
    "            calibrator.fit(X[:,self.index], y, sample_weight)\n",
    "            a, b = calibrator.coef_.ravel()[0], 0\n",
    "\n",
    "        c = calibrator.intercept_[0]\n",
    "        m = minimize_scalar(lambda m : np.abs(b*np.log(1.-m)-a*np.log(m)-c),\n",
    "                            bounds=[0, 1], method='Bounded').x\n",
    "\n",
    "        # Store related parameters\n",
    "        values = {\"a\": float(a), \"b\": float(b), \"c\": float(c), \"m\": float(m)}\n",
    "        self.params_ = namedtuple(\"Parameters\", values.keys())(**values)\n",
    "        \n",
    "        # Losses before and after calibration\n",
    "        self.calibrator_ = calibrator\n",
    "        y_calib = self.predict(y_proba)\n",
    "        self.losses_ = self.evaluate(y, y_proba, y_calib)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, y_proba):\n",
    "        \n",
    "        '''\n",
    "        Predict new values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_proba : array-like, shape (n_samples,)\n",
    "            Probability estimates.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_calib : array, shape (n_samples,)\n",
    "            The predicted values.\n",
    "            \n",
    "        '''\n",
    "        # Check if fit() was called\n",
    "        check_is_fitted(self, [\"params_\", \"calibrator_\"])\n",
    "\n",
    "        # Validate inputs\n",
    "        y_proba, _ = self.check_y_inputs(y_proba, None)\n",
    "        X = self.__transform__(y_proba)\n",
    "        y_calib = self.calibrator_.predict_proba(X[:,self.index])[:,1]\n",
    "        \n",
    "        return y_calib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56740425",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AM_BetaCal(BaseEstimator, RegressorMixin, ValidateParams, EvaluateLoss):\n",
    "    \n",
    "    '''\n",
    "    Beta regression model with two parameters (a and m, fixing a = b)\n",
    "    introduced in Kull, M., Silva Filho, T.M. and Flach, P. Beta \n",
    "    calibration:a well-founded and easily implemented improvement on \n",
    "    logistic calibration for binary classifiers. AISTATS 2017.\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    This implementation is adapted from the `betacal` Python library \n",
    "    (https://github.com/dirmeier/betacal) with modifications.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    params_ : namedtuple\n",
    "        Model coefficients: a, b, c (intercept), and m (midpoint).\n",
    "        \n",
    "    calibrator_ : sklearn.linear_model.LogisticRegression\n",
    "        Internal logistic regression used to train the model.\n",
    "        \n",
    "    losses_ : namedtuple\n",
    "        Pre and post calibration losses (Brier score and log-loss).\n",
    "\n",
    "    '''\n",
    "    \n",
    "    def __transform__(self, y_proba):\n",
    "        \n",
    "        '''\n",
    "        Transform probabilities into 2D-array for logistic regression.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y_proba : array-like, shape (n_samples,)\n",
    "            Probability estimates.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        X : array-like, shape (n_samples, 1)\n",
    "            Transformed X.\n",
    "        \n",
    "        '''\n",
    "        eps = np.finfo(float).eps\n",
    "        X = np.clip(y_proba, eps, 1 - eps).reshape(-1,1)\n",
    "        X = np.log(X / (1. - X))\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def fit(self, y_proba, y_true, sample_weight=None):\n",
    "        \n",
    "        '''\n",
    "        Fit the model using y_proba, y_true as training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "         y_proba : array-like, shape (n_samples,)\n",
    "            Probability estimates.\n",
    "            \n",
    "        y_true : array-like, shape (n_samples,)\n",
    "            Training data.\n",
    "\n",
    "        sample_weight : array-like, shape = (n_samples,), default=None \n",
    "            Sample weights. If None, then samples are equally weighted.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of self.\n",
    "            \n",
    "        '''\n",
    "        # Validate inputs\n",
    "        y_proba, y = self.check_y_inputs(y_proba, y_true)\n",
    "        X = self.__transform__(y_proba)\n",
    "     \n",
    "        # Initial model fitting\n",
    "        kwargs = dict(C=1e12, penalty='l2', solver='lbfgs', fit_intercept=True)\n",
    "        calibrator = LogisticRegression(**kwargs).fit(X, y, sample_weight)\n",
    "        a = calibrator.coef_.ravel()[0]\n",
    "        b = a \n",
    "        c = calibrator.intercept_[0]  \n",
    "        m = 1.0 / (1.0 + np.exp(c / a)) \n",
    "\n",
    "        # Store related parameters\n",
    "        values = {\"a\": float(a), \"b\": float(b), \"c\": float(c), \"m\": float(m)}\n",
    "        self.params_ = namedtuple(\"Parameters\", values.keys())(**values)\n",
    "        \n",
    "        # Losses before and after calibration\n",
    "        self.calibrator_ = calibrator\n",
    "        y_calib = self.predict(y_proba)\n",
    "        self.losses_ = self.evaluate(y, y_proba, y_calib)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, y_proba):\n",
    "        \n",
    "        '''\n",
    "        Predict new values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_proba : array-like, shape (n_samples,)\n",
    "            Probability estimates.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_calib : array, shape (n_samples,)\n",
    "            The predicted values.\n",
    "            \n",
    "        '''\n",
    "        # Check if fit() was called\n",
    "        check_is_fitted(self, [\"params_\", \"calibrator_\"])\n",
    "            \n",
    "        # Validate inputs\n",
    "        y_proba, _ = self.check_y_inputs(y_proba, None)\n",
    "        X = self.__transform__(y_proba)\n",
    "        y_calib = self.calibrator_.predict_proba(X)[:,1]\n",
    "        \n",
    "        return y_calib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84ba47dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AB_BetaCal(BaseEstimator, RegressorMixin, ValidateParams, EvaluateLoss):\n",
    "    \n",
    "    '''\n",
    "    Beta regression model with two parameters (a and b, fixing m = 0.5)\n",
    "    introduced in Kull, M., Silva Filho, T.M. and Flach, P. Beta \n",
    "    calibration: a well-founded and easily implemented improvement on \n",
    "    logistic calibration for binary classifiers. AISTATS 2017.\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    This implementation is adapted from the `betacal` Python library \n",
    "    (https://github.com/dirmeier/betacal) with modifications.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    params_ : namedtuple\n",
    "        Model coefficients: a, b, c (intercept), and m (midpoint).\n",
    "        \n",
    "    calibrator_ : sklearn.linear_model.LogisticRegression\n",
    "        Internal logistic regression used to train the model.\n",
    "        \n",
    "    losses_ : namedtuple\n",
    "        Pre and post calibration losses (Brier score and log-loss).\n",
    "\n",
    "    '''\n",
    "    \n",
    "    def __transform__(self, y_proba):\n",
    "        \n",
    "        '''\n",
    "        Transform probabilities into 2D-array for logistic regression.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y_proba : array-like, shape (n_samples,)\n",
    "            Probability estimates.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        X : array-like, shape (n_samples, 2)\n",
    "            Transformed X.\n",
    "        \n",
    "        '''\n",
    "        eps = np.finfo(float).eps\n",
    "        X = np.clip(y_proba, eps, 1 - eps).reshape(-1,1)\n",
    "        X = np.hstack((X, 1. - X))\n",
    "        X = np.log(2 * X)\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def fit(self, y_proba, y_true, sample_weight=None):\n",
    "        \n",
    "        '''\n",
    "        Fit the model using y_proba, y_true as training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_proba : array-like, shape (n_samples,)\n",
    "            Probability estimates.\n",
    "            \n",
    "        y_true : array-like, shape (n_samples,)\n",
    "            Training data.\n",
    "\n",
    "        sample_weight : array-like, shape = (n_samples,), default=None \n",
    "            Sample weights. If None, then samples are equally weighted.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of self.\n",
    "            \n",
    "        '''\n",
    "        # Validate inputs\n",
    "        y_proba, y = self.check_y_inputs(y_proba, y_true)\n",
    "        X = self.__transform__(y_proba)\n",
    "     \n",
    "        # Initial model fitting\n",
    "        kwargs = dict(C=1e12, penalty='l2', solver='lbfgs', fit_intercept=False)\n",
    "        calibrator = LogisticRegression(**kwargs).fit(X, y, sample_weight)\n",
    "        a, b = calibrator.coef_.ravel() * np.r_[1,-1]\n",
    "        c = 0\n",
    "        m = 0.5\n",
    "\n",
    "        # Store related parameters\n",
    "        values = {\"a\": float(a), \"b\": float(b), \"c\": float(c), \"m\": float(m)}\n",
    "        self.params_ = namedtuple(\"Parameters\", values.keys())(**values)\n",
    "        \n",
    "        # Losses before and after calibration\n",
    "        self.calibrator_ = calibrator\n",
    "        y_calib = self.predict(y_proba)\n",
    "        self.losses_ = self.evaluate(y, y_proba, y_calib)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, y_proba):\n",
    "        \n",
    "        '''\n",
    "        Predict new values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_proba : array-like, shape (n_samples,)\n",
    "            Probability estimates.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_calib : array, shape (n_samples,)\n",
    "            The predicted values.\n",
    "            \n",
    "        '''\n",
    "        # Check if fit() was called\n",
    "        check_is_fitted(self, [\"params_\", \"calibrator_\"])\n",
    "            \n",
    "        # Validate inputs\n",
    "        y_proba, _ = self.check_y_inputs(y_proba, None)\n",
    "        X = self.__transform__(y_proba)\n",
    "        y_calib = self.calibrator_.predict_proba(X)[:,1]\n",
    "        \n",
    "        return y_calib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ab41434",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid_Cal(BaseEstimator, RegressorMixin, ValidateParams, EvaluateLoss):\n",
    "    \n",
    "    '''\n",
    "    Platt’s scaling is a probability calibration method introduced by \n",
    "    John Platt (1999) to turn raw classifier scores into well\n",
    "    calibrated probabilities. Given a raw score $f(x)$, Platt’s \n",
    "    scaling models the probability of class 1 as: \n",
    "    \n",
    "                    P(y=1|x) = 1 / (1 + exp(A.(x) + B)\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    params_ : namedtuple\n",
    "        Model coefficients: a, b, c (intercept), and m (midpoint).\n",
    "        \n",
    "    calib_ : sklearn.linear_model.LogisticRegression\n",
    "        Internal logistic regression used to train the model.\n",
    "        \n",
    "    losses_ : namedtuple\n",
    "        Pre and post calibration losses (Brier score and log-loss).\n",
    "\n",
    "    '''\n",
    "    \n",
    "    def __transform__(self, y_proba):\n",
    "        \n",
    "        '''\n",
    "        Transform probabilities into 2D-array for logistic regression.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y_proba : array-like, shape (n_samples,)\n",
    "            Probability estimates.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        X : array-like, shape (n_samples, 2)\n",
    "            Transformed X.\n",
    "        \n",
    "        '''\n",
    "        eps = np.finfo(float).eps\n",
    "        X = np.clip(y_proba, eps, 1 - eps).reshape(-1,1)\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def fit(self, y_proba, y_true, sample_weight=None):\n",
    "        \n",
    "        '''\n",
    "        Fit the model using y_proba, y_true as training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_proba : array-like, shape (n_samples,)\n",
    "            Probability estimates.\n",
    "            \n",
    "        y_true : array-like, shape (n_samples,)\n",
    "            Training data.\n",
    "            \n",
    "        sample_weight : array-like, shape = (n_samples,), default=None \n",
    "            Sample weights. If None, then samples are equally weighted.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of self.\n",
    "            \n",
    "        '''\n",
    "        # Validate inputs\n",
    "        y_proba, y = self.check_y_inputs(y_proba, y_true)\n",
    "        X = self.__transform__(y_proba)\n",
    "     \n",
    "        # Initial model fitting\n",
    "        kwargs = dict(C=1e12, penalty='l2', solver='lbfgs', fit_intercept=True)\n",
    "        calibrator = LogisticRegression(**kwargs).fit(X, y, sample_weight)\n",
    "        a = calibrator.coef_.ravel()[0]\n",
    "        b = 0\n",
    "        c = calibrator.intercept_[0]  \n",
    "        m = -c/a\n",
    "\n",
    "        # Store related parameters\n",
    "        values = {\"a\": float(a), \"b\": 0, \"c\": float(c), \"m\": float(m)}\n",
    "        self.params_ = namedtuple(\"Parameters\", values.keys())(**values)\n",
    "        \n",
    "        # Losses before and after calibration\n",
    "        self.calibrator_ = calibrator\n",
    "        y_calib = self.predict(y_proba)\n",
    "        self.losses_ = self.evaluate(y, y_proba, y_calib)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, y_proba):\n",
    "        \n",
    "        '''\n",
    "        Predict new values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_proba : array-like, shape (n_samples,)\n",
    "            Probability estimates.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_calib : array, shape (n_samples,)\n",
    "            The predicted values.\n",
    "            \n",
    "        '''\n",
    "        # Check if fit() was called\n",
    "        check_is_fitted(self, [\"params_\", \"calibrator_\"])\n",
    "            \n",
    "        # Validate inputs\n",
    "        y_proba, _ = self.check_y_inputs(y_proba, None)\n",
    "        X = self.__transform__(y_proba)\n",
    "        y_calib = self.calibrator_.predict_proba(X)[:,1]\n",
    "        \n",
    "        return y_calib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a81f1a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaCalibration(BaseEstimator, RegressorMixin, ValidateParams, EvaluateLoss):\n",
    "    \n",
    "    '''\n",
    "    Wrapper class for the three Beta regression models introduced in \n",
    "    Kull, M., Silva Filho, T.M. and Flach, P. Beta calibration: a \n",
    "    well-founded and easily implemented improvement on logistic \n",
    "    calibration for binary classifiers. AISTATS 2017.\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    This implementation is adapted from the `betacal` Python library \n",
    "    (https://github.com/dirmeier/betacal) with modifications.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    parameters : {\"abm\", \"am\", \"ab\", \"sigmoid\"}, default=\"abm\"\n",
    "        Determines which parameters will be calculated by the model. \n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    calibrator_ : sklearn.linear_model.LogisticRegression\n",
    "        Internal calibration model corresponding to the chosen \n",
    "        parameterization.\n",
    "\n",
    "    '''\n",
    "   \n",
    "    def __init__(self, parameters=\"abm\"):\n",
    "        \n",
    "        params = {\"abm\": ABM_BetaCal(), \"am\": AM_BetaCal(), \n",
    "                  \"ab\": AB_BetaCal(), \"sigmoid\": Sigmoid_Cal()} \n",
    "        self.parameters = self.StrOptions(\"parameters\", parameters, params.keys())\n",
    "        self.calibrator_ = params[self.parameters]\n",
    "\n",
    "    def fit(self, y_proba, y_true, sample_weight=None):\n",
    "        \n",
    "        '''\n",
    "        Fit the model using y_proba, y_true as training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : array-like, shape (n_samples,)\n",
    "            Training data.\n",
    "\n",
    "        y_proba : array-like, shape (n_samples,)\n",
    "            Probability estimates.\n",
    "            \n",
    "        sample_weight : array-like, shape = (n_samples,), default=None \n",
    "            Sample weights. If None, then samples are equally weighted.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of self.\n",
    "            \n",
    "        '''\n",
    "        self.calibrator_.fit(y_proba, y_true, sample_weight)\n",
    "        return self\n",
    "\n",
    "    def predict(self, y_proba):\n",
    "        \n",
    "        '''\n",
    "        Predict new values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_proba : array-like, shape (n_samples,)\n",
    "            Probability estimates.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_calib : array, shape (n_samples,)\n",
    "            The predicted values.\n",
    "\n",
    "        '''\n",
    "        # Check if fit() was called\n",
    "        check_is_fitted(self, \"calibrator_\")\n",
    "        return self.calibrator_.predict(y_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6c4053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ef5dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2d99d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "95ea4440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters(a=0.4042760833483849, b=0.1573729226456932, c=-2.5330864622330997, m=0.9999940391390134)\n",
      "Losses(brier_score=[0.03543610488671236, 0.016995220755660515], log_loss=[0.13450623678484486, 0.08185930309450648])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQ4UlEQVR4nO3de4xd1XnG4fct5lZosQ0T5NimQxU3EVRcXIcSJYpaLFJsKmy1BJFesKglV6pbBaVp4iSVmlT9w1RVKUgVlQVJ7CpNcGiRrWCROg5R2qoQzCXmlpTBgdou4OFmGqwkJfmqL3xOD9MB5nLOnDPLv0fa2muvvfbZe3HG76xZe5+DI0IAgLb8VL8vAADQfYQ7ADSIcAeABhHuANAgwh0AGjRHA+C0006L4eHhfl8GAMwq995777MRMTSw4Z7Bvnv37n5fBgDMKraffL19TMsAQIMIdwBoEOEOAA0i3AGgQYQ7ADSIcAeAozHcbb/d9gMdy0u2r7E93/ZO24/Vel61TzfYHrG9x/bSGekJAGDi4R4R346I83KR9EuSDku6TdIGSbsiYkmuazutkLSklnWSbnyzcwAA+jsts1zS4xGRD86vkrS56nO9uspZvyVedZekubYXdPm6AQBvYLKfUL1S0uerfHpEPFXlp3O7ygsl7es4Zn/VHWn7Y7ZzVJ+LzjjjDE3V8Ibb1S9PbLy0b+cGgK6M3G0fJ+kySV8cuy+H6Lma6GvVMZsiYlkuQ0PjfjUCAGAGpmVyLv2+iHimtp85Mt1S64NVf0DS4o7jFlUdAGAAw/0DHVMyabukNVXO9baO+qvqqZkLJR3qmL4BAAzKnLvtkyRdLOn3O6o3Stpqe62kvMF6RdXvkLRS0kg9WXN1by4dADCtcI+IlyWdOqbuuXp6ZmzbnHtfP5HXBQD0Bp9QBYAGEe4A0CDCHQAaRLgDQIMIdwBoEOEOAA0i3AGgQYQ7ADSIcAeABhHuANAgwh0AGkS4A0CDCHcAaBDhDgANItwBoEGEOwA0iHAHgAYR7gDQIMIdABpEuANAgwh3ADhaw932XNu32v6W7Udtv8v2fNs7bT9W63nVNt1ge8T2HttLe94LAMCURu7XS7ojIt4h6VxJj0raIGlXRCzJdW2nFZKW1LJO0o0TPAcAYKbC3fYpkt4r6ebcjogfRMSLklZJ2lzNcr26ylm/JV51l6Qc9S/o1gUDALozcj9T0qikz9i+3/ZNtk+SdHpEPFVtns7tKi+UtK/j+P1V9xq219nencvoaL48AGAmw32OpJw3vzEizpf0cscUzI/lED1XkzlxRGyKiGW5DA0NTfrCAQDTC/ccee+PiLtr+9YK+2eOTLfU+mDtPyBpccfxi6oOADAo4R4ROeWyz/bbq2q5pEckbZe0pupyva3KWX9VPTVzoaRDHdM3AIAZkFMuE/FHkj5n+zhJeyVdXb8YttpeK+lJSVdU2x2SVkoakXS42gIABi3cI+IBScvG2bV8nLY5976+K1cHAJgSPqEKAA0i3AGgQYQ7ADSIcAeABhHuANAgwh0AGkS4A0CDCHcAaBDhDgANItwBoEGEOwA0iHAHgAYR7gDQIMIdABpEuANAgwh3AGgQ4Q4ADSLcAaBBhDsANIhwB4CjNdxtP2H7QdsP2N5ddfNt77T9WK3nVX26wfaI7T22l/a6EwCAqY/cfzUizouIZbW9QdKuiFiS69pOKyQtqWWdpBsncQ4AQJ+nZVZJ2lzlXK/uqN8Sr7pL0lzbC7pwrQCALod7SPpn2/faztF4Oj0inqry07ld5YWS9nUcu7/qXiNfJ6d4chkdHZ3o9QIAJmDORBpJek9EHLD9Fkk5v/6tzp05RLedvwAmLCI2ScpFy5Ytm9SxAIAujNwz2Gt9UNJtki6Q9MyR6ZZa576UbRd3HL6o6gAAgxLutk+y/TNHypLeJ+khSdslralmud5W5ay/qp6auVDSoY7pGwDAgEzL5Fz6bbaPtP+HiLjD9j2SttpeK+lJSVdU+x2SVkoakXRY0tW97QIAYNLhHhF7JZ07Tv1zkpaPU5/z5+vf7HUBAL3DJ1QBoEGEOwA0iHAHgAYR7gDQIMIdABpEuANAgwh3AGgQ4Q4ADSLcAaBBhDsANIhwB4AGEe4A0CDCHQAaRLgDQIMIdwBoEOEOAA0i3AGgQYQ7ADSIcAeABhHuANAgwh0AjuZwt32M7fttf6m2z7R9t+0R27fYPq7qj6/tkdo/3MsOAACmN3L/oKRHO7avlXRdRLxN0guS1lZ9rl+o+uuqHQBg0MLd9iJJl0q6qbYt6SJJt1aTzZJWV3lVbav2L6/2AIABG7n/jaSPSPpRbZ8q6cWIeKW290taWOVc78tC7T9U7V/D9jrbu3MZHR3tSmcAABMMd9u/LulgRNyrLoqITRGxLJehoaFuvjQAHPXmTKDNuyVdZnulpBMk/ayk6yXNtT2nRuc5bXOg2ud6cY7mc7+kUyQ9pwYNb7i9L+d9YmPOkAHANEbuEfGxiFgUEfnUy5WSvhoRvy3pTkmXV7M1krZVeXttq/Zn++hdFwAA3XzO/aOSPpSPPNac+s1Vn+tTq/5DkjZM4xwAgB5Ny/xERHxN0teqvFfSBeO0+Z6k90/lYgAA3cEnVAGgQYQ7ADSIcAeABhHuANAgwh0AGkS4A0CDCHcAaBDhDgANItwBoEGEOwA0iHAHgAYR7gDQIMIdABpEuANAgwh3AGgQ4Q4ADSLcAaBBhDsANIhwB4AGEe4AcDSGu+0TbH/D9jdtP2z7U1V/pu27bY/YvsX2cVV/fG2P1P7hmegIAGByI/fvS7ooIs6VdJ6kS2xfKOlaSddFxNskvSBpbbXP9QtVf121AwAMUrjHq75bm8fWEhn4km6t+s2SVld5VW2r9i+37d5cPgBgynPuto+x/YCkg5J2Snpc0osR8Uo12S9pYZVzvS8Ltf+QpFPHec11tnfnMjo6OpHLAAB0M9wj4ocRkVMyiyRdIOkdEz3BG7zmpohYlsvQ0NB0Xw4AMNWnZSLiRUl3SnqXpLm259SuDP0DVc714izU/lMkPTeZ8wAAev+0zJDtuVU+UdLFkh6tkL+8mq2RtK3K22tbtf+rOWk/zesEAEzCkZH3G1mQN0hz3r1+GWyNiC/ZfkTSF2z/haT7Jd1c7XP99/kopKTnJV05mQsCAMxAuEfEHknnj1O/t+bfx9Z/T9L7u3BtAIAp4hOqANAgwh0AGkS4A0CDCHcAaBDhDgANItwBoEGEOwA0iHAHgAYR7gDQIMIdABpEuANAgwh3AGgQ4Q4ADSLcAaBBhDsANIhwB4AGEe4A0CDCHQAaRLgDQIMIdwBoEOEOAEdjuNtebPtO24/Yftj2B6t+vu2dth+r9byqTzfYHrG9x/bSmegIAGByI/dXJP1xRJwl6UJJ621neYOkXRGxJNe1nVZIWlLLOkk3TuAcAICZDPeIeCoi7qvyf0t6VNJCSaskba5muV5d5azfEq+6S9Jc2wu6edEAgC7OudselnS+pLslnZ7BX7uezu0qZ/Dv6zhsf9WNfa11tnfnMjo6OpnLAAB0K9xtnyzpHyVdExEvde7LIXquJvpadcymiFiWy9DQ0GQOBQB0I9xtH1vB/rmI+KeqfubIdEutD1b9AUmLOw5fVHUAgAF6WsaSbs659oj4645d2yWtqXKut3XUX1VPzeQN2EMd0zcAgBkwZwJt3i3pdyU9aPuBqvu4pI2SttpeK+lJSVfUvh2SVkoakXRY0tU9vH4AwFTCPSL+NQfwr7N7+Tjtc+59/Zu9LgCgd/iEKgA0iHAHgAYR7gBwlN5QxYAZ3nB73879xMZL+3ZuABPHyB0AGkS4A0CDCHcAaBDhDgANItwBoEGEOwA0iHAHgAYR7gDQIMIdABpEuANAgwh3AGgQ4Q4ADSLcAaBBhDsANIhwB4AGEe4AcDSGu+1P2z5o+6GOuvm2d9p+rNbzqj7dYHvE9h7bS3vdAQDA1Ebun5V0yZi6DZJ2RcSSXNd2WiFpSS3rJN04gdcHAMx0uEfE1yU9P6Z6laTNVc716o76LfGquyTNtb2g2xcNAOjNnPvpEfFUlZ/O7SovlLSvo93+qvt/bK+zvTuX0dHRKV4GAKAnN1RziJ6rKRy3KSKW5TI0NDTdywAAdCHcnzky3VLrg1V/QNLijnaLqg4AMAvCfbukNVXO9baO+qvqqZkLJR3qmL4BAMyQOW/WwPbnJf2KpNNs5xz6n0naKGmr7bWSnpR0RTXfIWmlpBFJhyVd3fsuAAAmHe4R8YHX2bV8nLY5977+zV4TANBbfEIVAI7GkTvQaXjD7X057xMbL+3LeYHZipE7ADSIcAeABhHuANAgwh0AGsQNVcwK3MgFJoeROwA0iHAHgAYR7gDQIMIdABpEuANAgwh3AGgQ4Q4ADSLcAaBBhDsANIhPqAID+MnYfuJTuW1g5A4ADSLcAaBBhDsANIhwB4AG9eSGqu1LJF0v6RhJN0XExl6cB0Bb+GrnAQ532xnofyvpYkn7Jd1je3tEPNLtcwHovqPxCaHhPva5V79YejEtc4GkkYjYGxE/kPQFSat6cB4AwAxOyyyUtK9jO0fvvzy2ke11knJJ37X97Ume5zRJz6p99LMt9LMtp023n752Wuf/uYH7EFNEbJKUy5TY3h0Ry9Q4+tkW+tkWD3A/ezEtc0DS4o7tRVUHAJjF4X6PpCW2z7R9nKQrJW3vwXkAADM1LRMRr9j+Q0lfrkchPx0RD3f7PNOZ0pll6Gdb6GdbNmlAOSL6fQ0AgC7jE6oA0CDCHQAaNHDhnl9dkM+82x6xvWGc/cfbvqX23217uGPfx6o+j/81DbCp9tP2xbbvtf1grS/SgJvOe1r7z7Cdn4X4sAbYNH92z7H977Yfrvf2BLX3s3us7c3Vv0fz36sGmN+8n++1fZ/tvM94+Zh9a2w/Vssa9UPOuQ/KUjdgH5f085LySZtvSjprTJs/kPR3Vc4ncW6p8lnV/nhJZ9brHNPvPvWgn+dLemuVfzEfM+13f3rV1479t0r6oqQP97s/PXpP88GGPZLOre1TG/3Z/a38xHqVfzo/eZ+f/J/F/RyWdI6kLZIu76ifL2lvredVed5M92HQRu4T+eqC3N5c5fxHv9y2qz5/cL4fEd/J16nXG0RT7mdE3B8R/1X1+RTSiTlS0uCaznuaI6DVkvL97MUTV4PSz/dluEdEBkj+w3wuIn6o9vqZyXeS7fxldqKkPP4lzdJ+RsQTEZG/lH805ticNdgZEc9HxAtZlpRfpjijBi3cx/vqgoWv1yYfu5R0qEY6Ezm2hX52+k1J9+UvNA2uKffV9smSPirpUxp803lPfyGrbH+5/sz/iNrsZwb9y5KekvSfkv4qA1CDaeE08mQgsoj/h+osZftsSdfWqK9Vn5R0XUTkfLsalv8O3yPpnZIOS9qV91MiYpfakqPh/IvkrTVd8S+2v5Kj435fWIsGbeQ+ka8u+Emb+vPuFEnPzbKvPZhOP3M7298m6aqIyHnBQTadvuYXzv2l7ZybvUbSx+sDcq31M0d2X4+IZyMiw32HpKVqr585535HRPxPRByU9G+SBvJ7WTS9PBmMLOr3jYsxNyjyB2Fv3RA9chPj7DFt1o+5WbO1ymePuaG6d4BvSk2nn3Or/W/0ux+97uuYNp8c8Buq03lPcxR7X91kzNf5iqRLG+xnTrF9psonScr/x8M5s7Wf8X9tPzvODdXv1Ps6r8rzZ7wP/f6POM5/qJWS/qPuVH+i6v5c0mVVPqGenMgbpt/Iu9kdx36ijsuvD17R7770op+S/rTmLR/oWN7S7/706j2dLeE+3X5K+p26afxQ/rXSYj8lnVz1D1ew/8ks7+c766+ul+svk4c7jv296n8uV/fj+vn6AQBo0KDNuQMAuoBwB4AGEe4A0CDCHQAaRLgDQIMIdwBoEOEOAGrP/wJYnSiszRmzSAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = BetaCalibration(\"abm\")\n",
    "a.fit(probs_cal, y_cal)\n",
    "print(a.calibrator_.params_)\n",
    "print(a.calibrator_.losses_)\n",
    "_ = plt.hist(a.predict(probs_cal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "246753f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters(a=4.190402793717399, b=2.774682174703487, c=-5.596753143578118, m=0.8886780598411288)\n",
      "Losses(brier_score=[0.00471240377987703, 2.053316954652412e-05], log_loss=[0.022577843084050125, 0.00023258099091412956])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = BetaCalibration(\"abm\")\n",
    "a.fit(probs_eval, y_eval)\n",
    "print(a.calibrator_.params_)\n",
    "print(a.calibrator_.losses_)\n",
    "_ = plt.hist(a.predict(probs_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3d4464dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.predict(y_eval[y_eval==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b618cc38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "174a11f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "X = pd.read_csv('HL_online_fraud.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50966326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>raw_score</th>\n",
       "      <th>calibrated_score</th>\n",
       "      <th>quantiled_calibrated_score</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.005857</td>\n",
       "      <td>-0.343981</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.005847</td>\n",
       "      <td>-0.665472</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train</td>\n",
       "      <td>0.002743</td>\n",
       "      <td>0.005876</td>\n",
       "      <td>0.009397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.005840</td>\n",
       "      <td>-1.299295</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>0.005845</td>\n",
       "      <td>-0.824825</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset  raw_score  calibrated_score  quantiled_calibrated_score  target\n",
       "0   train   0.000780          0.005857                   -0.343981       0\n",
       "1   train   0.000535          0.005847                   -0.665472       0\n",
       "2   train   0.002743          0.005876                    0.009397       0\n",
       "3   train   0.000077          0.005840                   -1.299295       0\n",
       "4   train   0.000686          0.005845                   -0.824825       0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71a370a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_cal = X.loc[X[\"dataset\"]==\"test\",\"raw_score\"].values\n",
    "y_cal = X.loc[X[\"dataset\"]==\"test\",\"target\"].values\n",
    "probs_eval = X.loc[X[\"dataset\"]==\"train\",\"raw_score\"].values\n",
    "y_eval = X.loc[X[\"dataset\"]==\"train\",\"target\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55afa322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c734b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93d65f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b762c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe92518",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
