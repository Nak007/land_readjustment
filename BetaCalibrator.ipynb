{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e00e48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Available methods are the followings:\n",
    "[1] BetaCalibration\n",
    "[2] ABM_BetaCal\n",
    "[3] AB_BetaCal\n",
    "[4] AM_BetaCal\n",
    "[5] Sigmoid_Cal\n",
    "[6] ModifiedLogistic\n",
    "\n",
    "Authors: Danusorn Sitdhirasdr <danusorn.si@gmail.com>\n",
    "versionadded:: 20-09-2025\n",
    "\n",
    "'''\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, clone\n",
    "from sklearn.metrics import brier_score_loss, log_loss, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import column_or_1d, check_consistent_length\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.utils.validation import (check_array, check_is_fitted, \n",
    "                                      _check_sample_weight, check_X_y)\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from scipy.optimize import minimize_scalar, minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24d41e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = [\"BetaCalibration\",\n",
    "           \"ABM_BetaCal\",\n",
    "           \"AB_BetaCal\",\n",
    "           \"AM_BetaCal\", \n",
    "           \"Sigmoid_Cal\",\n",
    "           \"ModifiedLogistic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a557bbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidateParams:\n",
    "    \n",
    "    '''Validate parameters'''\n",
    "    \n",
    "    def Interval(self, Param, Value, dtype=int, \n",
    "                 left=None, right=None, closed=\"both\"):\n",
    "\n",
    "        '''\n",
    "        Validate numerical input.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Param : str\n",
    "            Parameter's name\n",
    "\n",
    "        Value : float or int\n",
    "            Parameter's value\n",
    "\n",
    "        dtype : {int, float}, default=int\n",
    "            The type of input.\n",
    "\n",
    "        left : float or int or None, default=None\n",
    "            The left bound of the interval. None means left bound is -∞.\n",
    "\n",
    "        right : float, int or None, default=None\n",
    "            The right bound of the interval. None means right bound is +∞.\n",
    "\n",
    "        closed : {\"left\", \"right\", \"both\", \"neither\"}\n",
    "            Whether the interval is open or closed. Possible choices are:\n",
    "            - \"left\": the interval is closed on the left and open on the \n",
    "              right. It is equivalent to the interval [ left, right ).\n",
    "            - \"right\": the interval is closed on the right and open on the \n",
    "              left. It is equivalent to the interval ( left, right ].\n",
    "            - \"both\": the interval is closed.\n",
    "              It is equivalent to the interval [ left, right ].\n",
    "            - \"neither\": the interval is open.\n",
    "              It is equivalent to the interval ( left, right ).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Value : float or int\n",
    "            Parameter's value\n",
    "\n",
    "        '''\n",
    "        Options = {\"left\"    : (np.greater_equal, np.less), # a<=x<b\n",
    "                   \"right\"   : (np.greater, np.less_equal), # a<x<=b\n",
    "                   \"both\"    : (np.greater_equal, np.less_equal), # a<=x<=b\n",
    "                   \"neither\" : (np.greater, np.less)} # a<x<b\n",
    "\n",
    "        f0, f1 = Options[closed]\n",
    "        c0 = \"[\" if f0.__name__.find(\"eq\")>-1 else \"(\" \n",
    "        c1 = \"]\" if f1.__name__.find(\"eq\")>-1 else \")\"\n",
    "        v0 = \"-∞\" if left is None else str(dtype(left))\n",
    "        v1 = \"+∞\" if right is None else str(dtype(right))\n",
    "        if left  is None: left  = -np.inf\n",
    "        if right is None: right = +np.inf\n",
    "        interval = \", \".join([c0+v0, v1+c1])\n",
    "        tuples = (Param, dtype.__name__, interval, Value)\n",
    "        err_msg = \"%s must be %s or in %s, got %s \" % tuples    \n",
    "\n",
    "        if isinstance(Value, dtype):\n",
    "            if not (f0(Value, left) & f1(Value, right)):\n",
    "                raise ValueError(err_msg)\n",
    "        else: raise ValueError(err_msg)\n",
    "        return Value\n",
    "\n",
    "    def StrOptions(self, Param, Value, options, dtype=str):\n",
    "\n",
    "        '''\n",
    "        Validate string or boolean inputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Param : str\n",
    "            Parameter's name\n",
    "            \n",
    "        Value : float or int\n",
    "            Parameter's value\n",
    "\n",
    "        options : set of str\n",
    "            The set of valid strings.\n",
    "\n",
    "        dtype : {str, bool}, default=str\n",
    "            The type of input.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Value : float or int\n",
    "            Parameter's value\n",
    "\n",
    "        '''\n",
    "        if Value not in options:\n",
    "            err_msg = f'{Param} ({dtype.__name__}) must be either '\n",
    "            for n,s in enumerate(options):\n",
    "                if n<len(options)-1: err_msg += f'\"{s}\", '\n",
    "                else: err_msg += f' or \"{s}\" , got %s'\n",
    "            raise ValueError(err_msg % Value)\n",
    "        return Value\n",
    "    \n",
    "    def check_range(self, param0, param1):\n",
    "        \n",
    "        '''\n",
    "        Validate number range.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        param0 : tuple(str, float)\n",
    "            A lower bound parameter e.g. (\"name\", -100.)\n",
    "            \n",
    "        param1 : tuple(str, float)\n",
    "            An upper bound parameter e.g. (\"name\", 100.)\n",
    "            \n",
    "        '''\n",
    "        if param0[1] >= param1[1]:\n",
    "            raise ValueError(f\"`{param0[0]}` ({param0[1]}) must be less\"\n",
    "                             f\" than `{param1[0]}` ({param1[1]}).\")\n",
    "            \n",
    "    def check_y_inputs(self, y_proba, y_true=None):\n",
    "        \n",
    "        '''\n",
    "        Validate inputs for calibration models.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_proba : array-like, shape (n_samples,)\n",
    "            Predicted probabilities ranges from 0 to 1.\n",
    "            \n",
    "        y_true : array-like, shape (n_samples,), default=None\n",
    "            True binary labels (0/1).\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y_proba : ndarray shape (n_samples,)\n",
    "            Validated probabilities.\n",
    "            \n",
    "        y_true : ndarray shape (n_samples,)\n",
    "            Validated labels. This returns None when `y_ture` is not \n",
    "            provided.\n",
    "            \n",
    "        '''\n",
    "        # Ensure shapes\n",
    "        y_proba = column_or_1d(y_proba)\n",
    "        y_proba = check_array(y_proba, ensure_2d=False, dtype=float)\n",
    "\n",
    "        # Probability range\n",
    "        eps = np.finfo(float).eps\n",
    "        y_proba = np.clip(y_proba, eps, 1 - eps)\n",
    "        \n",
    "        if y_true is not None:\n",
    "            \n",
    "            # Check lengths\n",
    "            y_true = column_or_1d(y_true)\n",
    "            check_consistent_length(y_proba, y_true)\n",
    "\n",
    "            # Check that y_true only contains {0,1}\n",
    "            unique_labels = np.unique(y_true)\n",
    "            if not np.all(np.isin(unique_labels, [0, 1])):\n",
    "                raise ValueError(\"`y_true` must be binary (0 or 1).\")\n",
    "            return y_proba, y_true\n",
    "        \n",
    "        else: return y_proba, None\n",
    "        \n",
    "    def check_class_weight(self, class_weight):\n",
    "        \n",
    "        '''\n",
    "        Validate the `class_weight` parameter.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        class_weight : {dict, \"balanced\"} or None\n",
    "            Weights associated with classes in the form `{label: weight}`.\n",
    "            If set to \"balanced\", the weights are automatically adjusted\n",
    "            inversely proportional to class frequencies.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        class_weight : dict, str or None\n",
    "            Normalized class_weight.\n",
    "            \n",
    "        '''\n",
    "         # Validate the `class_weight` parameter.\n",
    "        if class_weight is None:\n",
    "            return None\n",
    "        elif isinstance(class_weight, str):\n",
    "            args = (\"class_weight\", class_weight, [\"balanced\"], str)\n",
    "            return self.StrOptions(*args)\n",
    "        elif isinstance(class_weight, dict):\n",
    "            return class_weight\n",
    "        else:raise ValueError(f\"Invalid value for `class_weight`: {class_weight}. \"\n",
    "                              f\"Expected dict, 'balanced', or None.\")\n",
    "            \n",
    "    def check_estimator(self, estimator):\n",
    "        \n",
    "        '''\n",
    "        Validate that an estimator follows sklearn interface.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        estimator : object\n",
    "            Estimator instance.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If estimator does not implement required methods/attributes.\n",
    "            \n",
    "        '''\n",
    "        for m in [\"get_params\", \"set_params\", \"fit\", \"predict\", \"predict_proba\"]:\n",
    "            if not callable(getattr(estimator, m, None)):\n",
    "                raise ValueError(f\"Estimator missing required method `{m}()`.\")\n",
    "\n",
    "        if estimator.get_params().get(\"fit_intercept\", None) is None:\n",
    "            raise ValueError(\"Estimator missing required parameter \"\n",
    "                             \"`fit_intercept` in get_params().\")\n",
    "        \n",
    "        # Fit model with dummy X and y\n",
    "        N, rnd = 10, np.random.RandomState(0)\n",
    "        estimator.fit(rnd.rand(N).reshape(-1,1), \n",
    "                      rnd.randint(0, 2, size=N))\n",
    "        \n",
    "        for a in [\"coef_\", \"intercept_\"]:\n",
    "            if getattr(estimator, a, None) is None:\n",
    "                raise ValueError(f\"Estimator missing required attribute \"\n",
    "                                 f\"`{attr}` after fit().\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68ea8603",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateLoss:\n",
    "    \n",
    "    '''\n",
    "    Utility class to evaluate model performance before and after \n",
    "    calibration.\n",
    "\n",
    "    Metrics:\n",
    "        - Brier score\n",
    "        - Log loss\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    @staticmethod\n",
    "    def _gini(y_true, y_proba):\n",
    "        \n",
    "        '''\n",
    "        Compute Gini coefficient from ROC AUC.\n",
    "        '''\n",
    "        return 2 * roc_auc_score(y_true, y_proba) - 1\n",
    "\n",
    "    def evaluate(self, y_true, y_proba, y_calib):\n",
    "        \n",
    "        '''\n",
    "        Evaluate loss metrics before and after calibration.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : array-like of shape (n_samples,)\n",
    "            Ground truth binary labels.\n",
    "\n",
    "        y_proba : array-like of shape (n_samples,)\n",
    "            Predicted probabilities before calibration.\n",
    "\n",
    "        y_calib : array-like of shape (n_samples,)\n",
    "            Predicted probabilities after calibration.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        losses : namedtuple\n",
    "            Pre and post calibration losses (Brier score and log-loss)\n",
    "            \n",
    "        '''\n",
    "        a, b = (y_true, y_proba), (y_true, y_calib)\n",
    "        losses = {\"brier_score\": [float(brier_score_loss(*a)), \n",
    "                                  float(brier_score_loss(*b))],\n",
    "                  \"log_loss\"   : [float(log_loss(*a)), \n",
    "                                  float(log_loss(*b))]}\n",
    "        \n",
    "        return namedtuple(\"Losses\", losses.keys())(**losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0082cb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABM_BetaCal(BaseEstimator, RegressorMixin, ValidateParams, EvaluateLoss):\n",
    "    \n",
    "    '''\n",
    "    Beta regression model with three parameters introduced in \n",
    "    Kull, M., Silva Filho, T.M. and Flach, P. (2017).\n",
    "    Beta calibration: a well-founded and easily implemented \n",
    "    improvement on logistic calibration for binary classifiers.\n",
    "    AISTATS 2017.\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    This implementation is adapted from the `betacal` Python library \n",
    "    (https://github.com/dirmeier/betacal) with modifications.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator object, default=None\n",
    "        Base estimator used for calibration. It must be compatible \n",
    "        with the scikit-learn regressor interface (supporting parameter \n",
    "        management, fitting, prediction, and probability estimation).  \n",
    "        If None, defaults to LogisticRegression with parameters \n",
    "        `C=1e12`, and `fit_intercept=True`.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    params_ : namedtuple\n",
    "        Model parameters:\n",
    "        - a : coefficient for log(p)\n",
    "        - b : coefficient for log(1 - p)\n",
    "        - c : intercept\n",
    "        - m : midpoint\n",
    "        \n",
    "    calibrator_ : sklearn.linear_model.LogisticRegression\n",
    "        Internal logistic regression used to train the model.\n",
    "        \n",
    "    losses_ : namedtuple\n",
    "        Pre and post calibration losses (Brier score and log-loss).\n",
    "\n",
    "    '''\n",
    "    def __init__(self, estimator=None):\n",
    "        \n",
    "        if estimator is None:\n",
    "            kwargs = dict(C=1e12, fit_intercept=True)\n",
    "            self.estimator = LogisticRegression(**kwargs)\n",
    "        else:\n",
    "            self.check_estimator(estimator)\n",
    "            estimator.set_params(**{\"fit_intercept\":False})\n",
    "            self.estimator = estimator\n",
    "        \n",
    "    def __transform__(self, y_proba):\n",
    "        \n",
    "        '''\n",
    "        Transform probabilities into 2D-array for logistic regression.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y_proba : array-like, shape (n_samples,)\n",
    "            Probability estimates.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        X : array-like, shape (n_samples, 2)\n",
    "            Transformed X.\n",
    "        \n",
    "        '''\n",
    "        eps = np.finfo(float).eps\n",
    "        X = np.clip(y_proba, eps, 1 - eps).reshape(-1,1)\n",
    "        X = np.log(np.hstack((X, 1.0 - X)))\n",
    "        X[:, 1] *= -1\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def fit(self, y_proba, y_true, sample_weight=None):\n",
    "        \n",
    "        '''\n",
    "        Fit the model using y_proba, y_true as training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_proba : array-like, shape (n_samples,)\n",
    "            Probability estimates.\n",
    "            \n",
    "        y_true : array-like, shape (n_samples,)\n",
    "            Training data.\n",
    "\n",
    "        sample_weight : array-like, shape = (n_samples,), default=None \n",
    "            Sample weights. If None, then samples are equally weighted.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of self.\n",
    "            \n",
    "        '''\n",
    "        # Validate inputs\n",
    "        y_proba, y = self.check_y_inputs(y_proba, y_true)\n",
    "        X = self.__transform__(y_proba)\n",
    "     \n",
    "        # Initial model fitting\n",
    "        self.calibrator_ = clone(self.estimator).fit(X, y, sample_weight)\n",
    "        a, b = self.calibrator_.coef_.ravel()\n",
    "        self.index = [0,1]\n",
    "        \n",
    "        # Adjust if coefficients are negative\n",
    "        if a < 0:\n",
    "            self.index = [1]\n",
    "            self.calibrator_.fit(X[:,self.index], y, sample_weight)\n",
    "            a, b = 0, self.calibrator_.coef_.ravel()[0]\n",
    "        elif b < 0:\n",
    "            self.index = [0]\n",
    "            self.calibrator_.fit(X[:,self.index], y, sample_weight)\n",
    "            a, b = self.calibrator_.coef_.ravel()[0], 0\n",
    "\n",
    "        c = self.calibrator_.intercept_[0]\n",
    "        m = minimize_scalar(lambda m : np.abs(b*np.log(1.-m)-a*np.log(m)-c),\n",
    "                            bounds=[0, 1], method='Bounded').x\n",
    "\n",
    "        # Store related parameters\n",
    "        values = {\"a\": float(a), \"b\": float(b), \"c\": float(c), \"m\": float(m)}\n",
    "        self.params_ = namedtuple(\"Parameters\", values.keys())(**values)\n",
    "        \n",
    "        # Losses before and after calibration\n",
    "        y_calib = self.predict(y_proba)\n",
    "        self.losses_ = self.evaluate(y, y_proba, y_calib)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, y_proba):\n",
    "        \n",
    "        '''\n",
    "        Predict new values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_proba : array-like, shape (n_samples,)\n",
    "            Probability estimates.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_calib : array, shape (n_samples,)\n",
    "            The predicted values.\n",
    "            \n",
    "        '''\n",
    "        # Check if fit() was called\n",
    "        check_is_fitted(self, [\"params_\", \"calibrator_\"])\n",
    "\n",
    "        # Validate inputs\n",
    "        y_proba, _ = self.check_y_inputs(y_proba, None)\n",
    "        X = self.__transform__(y_proba)\n",
    "        y_calib = self.calibrator_.predict_proba(X[:,self.index])[:,1]\n",
    "        \n",
    "        return y_calib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56740425",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AM_BetaCal(BaseEstimator, RegressorMixin, ValidateParams, EvaluateLoss):\n",
    "    \n",
    "    '''\n",
    "    Beta regression model with two parameters (a and m, fixing a = b)\n",
    "    introduced in Kull, M., Silva Filho, T.M. and Flach, P. Beta \n",
    "    calibration:a well-founded and easily implemented improvement on \n",
    "    logistic calibration for binary classifiers. AISTATS 2017.\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    This implementation is adapted from the `betacal` Python library \n",
    "    (https://github.com/dirmeier/betacal) with modifications.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator object, default=None\n",
    "        Base estimator used for calibration. It must be compatible \n",
    "        with the scikit-learn regressor interface (supporting parameter \n",
    "        management, fitting, prediction, and probability estimation).  \n",
    "        If None, defaults to LogisticRegression with parameters \n",
    "        `C=1e12`, and `fit_intercept=True`.\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    params_ : namedtuple\n",
    "        Model parameters:\n",
    "        - a : coefficient for log(p / (1 - p))\n",
    "        - b : a\n",
    "        - c : intercept\n",
    "        - m : midpoint\n",
    "        \n",
    "    calibrator_ : sklearn.linear_model.LogisticRegression\n",
    "        Internal logistic regression used to train the model.\n",
    "        \n",
    "    losses_ : namedtuple\n",
    "        Pre and post calibration losses (Brier score and log-loss).\n",
    "\n",
    "    '''\n",
    "    def __init__(self, estimator=None):\n",
    "        \n",
    "        if estimator is None:\n",
    "            kwargs = dict(C=1e12, fit_intercept=True)\n",
    "            self.estimator = LogisticRegression(**kwargs)\n",
    "        else:\n",
    "            self.check_estimator(estimator)\n",
    "            estimator.set_params(**{\"fit_intercept\":True})\n",
    "            self.estimator = estimator\n",
    "        \n",
    "    def __transform__(self, y_proba):\n",
    "        \n",
    "        '''\n",
    "        Transform probabilities into 2D-array for logistic regression.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y_proba : array-like, shape (n_samples,)\n",
    "            Probability estimates.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        X : array-like, shape (n_samples, 1)\n",
    "            Transformed X.\n",
    "        \n",
    "        '''\n",
    "        eps = np.finfo(float).eps\n",
    "        X = np.clip(y_proba, eps, 1 - eps).reshape(-1,1)\n",
    "        X = np.log(X / (1. - X))\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def fit(self, y_proba, y_true, sample_weight=None):\n",
    "        \n",
    "        '''\n",
    "        Fit the model using y_proba, y_true as training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "         y_proba : array-like, shape (n_samples,)\n",
    "            Probability estimates.\n",
    "            \n",
    "        y_true : array-like, shape (n_samples,)\n",
    "            Training data.\n",
    "\n",
    "        sample_weight : array-like, shape = (n_samples,), default=None \n",
    "            Sample weights. If None, then samples are equally weighted.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of self.\n",
    "            \n",
    "        '''\n",
    "        # Validate inputs\n",
    "        y_proba, y = self.check_y_inputs(y_proba, y_true)\n",
    "        X = self.__transform__(y_proba)\n",
    "     \n",
    "        # Initial model fitting\n",
    "        self.calibrator_ = clone(self.estimator).fit(X, y, sample_weight)\n",
    "        a = self.calibrator_.coef_.ravel()[0]\n",
    "        b = a \n",
    "        c = self.calibrator_.intercept_[0]  \n",
    "        m = 1. / (1. + np.exp(c / a)) \n",
    "\n",
    "        # Store related parameters\n",
    "        values = {\"a\": float(a), \"b\": float(b), \"c\": float(c), \"m\": float(m)}\n",
    "        self.params_ = namedtuple(\"Parameters\", values.keys())(**values)\n",
    "        \n",
    "        # Losses before and after calibration\n",
    "        y_calib = self.predict(y_proba)\n",
    "        self.losses_ = self.evaluate(y, y_proba, y_calib)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, y_proba):\n",
    "        \n",
    "        '''\n",
    "        Predict new values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_proba : array-like, shape (n_samples,)\n",
    "            Probability estimates.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_calib : array, shape (n_samples,)\n",
    "            The predicted values.\n",
    "            \n",
    "        '''\n",
    "        # Check if fit() was called\n",
    "        check_is_fitted(self, [\"params_\", \"calibrator_\"])\n",
    "            \n",
    "        # Validate inputs\n",
    "        y_proba, _ = self.check_y_inputs(y_proba, None)\n",
    "        X = self.__transform__(y_proba)\n",
    "        y_calib = self.calibrator_.predict_proba(X)[:,1]\n",
    "        \n",
    "        return y_calib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84ba47dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AB_BetaCal(BaseEstimator, RegressorMixin, ValidateParams, EvaluateLoss):\n",
    "    \n",
    "    '''\n",
    "    Beta regression model with two parameters (a and b, fixing m = 0.5)\n",
    "    introduced in Kull, M., Silva Filho, T.M. and Flach, P. Beta \n",
    "    calibration: a well-founded and easily implemented improvement on \n",
    "    logistic calibration for binary classifiers. AISTATS 2017.\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    This implementation is adapted from the `betacal` Python library \n",
    "    (https://github.com/dirmeier/betacal) with modifications.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator object, default=None\n",
    "        Base estimator used for calibration. It must be compatible \n",
    "        with the scikit-learn regressor interface (supporting parameter \n",
    "        management, fitting, prediction, and probability estimation).  \n",
    "        If None, defaults to LogisticRegression with parameters \n",
    "        `C=1e12`, and `fit_intercept=True`.\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    params_ : namedtuple\n",
    "        Model parameters:\n",
    "        - a : coefficient for log(2^p) \n",
    "        - b : coefficient for log(2^(1-p)) \n",
    "        - c : 0.\n",
    "        - m : midpoint\n",
    "                \n",
    "    calibrator_ : sklearn.linear_model.LogisticRegression\n",
    "        Internal logistic regression used to train the model.\n",
    "        \n",
    "    losses_ : namedtuple\n",
    "        Pre and post calibration losses (Brier score and log-loss).\n",
    "\n",
    "    '''\n",
    "    def __init__(self, estimator=None):\n",
    "        \n",
    "        if estimator is None:\n",
    "            kwargs = dict(C=1e12, fit_intercept=False)\n",
    "            self.estimator = LogisticRegression(**kwargs)\n",
    "        else:\n",
    "            self.check_estimator(estimator)\n",
    "            estimator.set_params(**{\"fit_intercept\":False})\n",
    "            self.estimator = estimator\n",
    "            \n",
    "    def __transform__(self, y_proba):\n",
    "        \n",
    "        '''\n",
    "        Transform probabilities into 2D-array for logistic regression.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y_proba : array-like, shape (n_samples,)\n",
    "            Probability estimates.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        X : array-like, shape (n_samples, 2)\n",
    "            Transformed X.\n",
    "        \n",
    "        '''\n",
    "        eps = np.finfo(float).eps\n",
    "        X = np.clip(y_proba, eps, 1 - eps).reshape(-1,1)\n",
    "        X = np.hstack((X, 1. - X))\n",
    "        X = np.log(2 * X)\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def fit(self, y_proba, y_true, sample_weight=None):\n",
    "        \n",
    "        '''\n",
    "        Fit the model using y_proba, y_true as training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_proba : array-like, shape (n_samples,)\n",
    "            Probability estimates.\n",
    "            \n",
    "        y_true : array-like, shape (n_samples,)\n",
    "            Training data.\n",
    "\n",
    "        sample_weight : array-like, shape = (n_samples,), default=None \n",
    "            Sample weights. If None, then samples are equally weighted.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of self.\n",
    "            \n",
    "        '''\n",
    "        # Validate inputs\n",
    "        y_proba, y = self.check_y_inputs(y_proba, y_true)\n",
    "        X = self.__transform__(y_proba)\n",
    "     \n",
    "        # Initial model fitting\n",
    "        self.calibrator_ = clone(self.estimator).fit(X, y, sample_weight)\n",
    "        a, b = self.calibrator_.coef_.ravel() * np.r_[1,-1]\n",
    "        c = 0\n",
    "        m = 0.5\n",
    "\n",
    "        # Store related parameters\n",
    "        values = {\"a\": float(a), \"b\": float(b), \"c\": float(c), \"m\": float(m)}\n",
    "        self.params_ = namedtuple(\"Parameters\", values.keys())(**values)\n",
    "        \n",
    "        # Losses before and after calibration\n",
    "        y_calib = self.predict(y_proba)\n",
    "        self.losses_ = self.evaluate(y, y_proba, y_calib)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, y_proba):\n",
    "        \n",
    "        '''\n",
    "        Predict new values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_proba : array-like, shape (n_samples,)\n",
    "            Probability estimates.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_calib : array, shape (n_samples,)\n",
    "            The predicted values.\n",
    "            \n",
    "        '''\n",
    "        # Check if fit() was called\n",
    "        check_is_fitted(self, [\"params_\", \"calibrator_\"])\n",
    "            \n",
    "        # Validate inputs\n",
    "        y_proba, _ = self.check_y_inputs(y_proba, None)\n",
    "        X = self.__transform__(y_proba)\n",
    "        y_calib = self.calibrator_.predict_proba(X)[:,1]\n",
    "        \n",
    "        return y_calib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ab41434",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid_Cal(BaseEstimator, RegressorMixin, ValidateParams, EvaluateLoss):\n",
    "    \n",
    "    '''\n",
    "    Platt’s scaling is a probability calibration method introduced by \n",
    "    John Platt (1999) to turn raw classifier scores into well\n",
    "    calibrated probabilities. Given a raw score $f(x)$, Platt’s \n",
    "    scaling models the probability of class 1 as: \n",
    "    \n",
    "                    P(y=1|x) = 1 / (1 + exp(A.(x) + B)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator object, default=None\n",
    "        Base estimator used for calibration. It must be compatible \n",
    "        with the scikit-learn regressor interface (supporting parameter \n",
    "        management, fitting, prediction, and probability estimation).  \n",
    "        If None, defaults to LogisticRegression with parameters \n",
    "        `C=1e12`, and `fit_intercept=True`.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    params_ : namedtuple\n",
    "        Model parameters:\n",
    "        - a : coefficient for p\n",
    "        - b : a\n",
    "        - c : intercept\n",
    "        - m : midpoint\n",
    "        \n",
    "    calib_ : sklearn.linear_model.LogisticRegression\n",
    "        Internal logistic regression used to train the model.\n",
    "        \n",
    "    losses_ : namedtuple\n",
    "        Pre and post calibration losses (Brier score and log-loss).\n",
    "\n",
    "    '''\n",
    "    def __init__(self, estimator=None):\n",
    "        \n",
    "        if estimator is None:\n",
    "            kwargs = dict(C=1e12, fit_intercept=True)\n",
    "            self.estimator = LogisticRegression(**kwargs)\n",
    "        else:\n",
    "            self.check_estimator(estimator)\n",
    "            estimator.set_params(**{\"fit_intercept\":True})\n",
    "            self.estimator = estimator\n",
    "            \n",
    "    def __transform__(self, y_proba):\n",
    "        \n",
    "        '''\n",
    "        Transform probabilities into 2D-array for logistic regression.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y_proba : array-like, shape (n_samples,)\n",
    "            Probability estimates.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        X : array-like, shape (n_samples, 2)\n",
    "            Transformed X.\n",
    "        \n",
    "        '''\n",
    "        eps = np.finfo(float).eps\n",
    "        X = np.clip(y_proba, eps, 1 - eps).reshape(-1,1)\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def fit(self, y_proba, y_true, sample_weight=None):\n",
    "        \n",
    "        '''\n",
    "        Fit the model using y_proba, y_true as training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_proba : array-like, shape (n_samples,)\n",
    "            Probability estimates.\n",
    "            \n",
    "        y_true : array-like, shape (n_samples,)\n",
    "            Training data.\n",
    "            \n",
    "        sample_weight : array-like, shape = (n_samples,), default=None \n",
    "            Sample weights. If None, then samples are equally weighted.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of self.\n",
    "            \n",
    "        '''\n",
    "        # Validate inputs\n",
    "        y_proba, y = self.check_y_inputs(y_proba, y_true)\n",
    "        X = self.__transform__(y_proba)\n",
    "     \n",
    "        # Initial model fitting\n",
    "        self.calibrator_ = clone(self.estimator).fit(X, y, sample_weight)\n",
    "        a = self.calibrator_.coef_.ravel()[0]\n",
    "        b = a\n",
    "        c = self.calibrator_.intercept_[0]  \n",
    "        m = -c/a\n",
    "\n",
    "        # Store related parameters\n",
    "        values = {\"a\": float(a), \"b\": float(b), \"c\": float(c), \"m\": float(m)}\n",
    "        self.params_ = namedtuple(\"Parameters\", values.keys())(**values)\n",
    "        \n",
    "        # Losses before and after calibration\n",
    "        y_calib = self.predict(y_proba)\n",
    "        self.losses_ = self.evaluate(y, y_proba, y_calib)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, y_proba):\n",
    "        \n",
    "        '''\n",
    "        Predict new values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_proba : array-like, shape (n_samples,)\n",
    "            Probability estimates.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_calib : array, shape (n_samples,)\n",
    "            The predicted values.\n",
    "            \n",
    "        '''\n",
    "        # Check if fit() was called\n",
    "        check_is_fitted(self, [\"params_\", \"calibrator_\"])\n",
    "            \n",
    "        # Validate inputs\n",
    "        y_proba, _ = self.check_y_inputs(y_proba, None)\n",
    "        X = self.__transform__(y_proba)\n",
    "        y_calib = self.calibrator_.predict_proba(X)[:,1]\n",
    "        \n",
    "        return y_calib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a81f1a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaCalibration(BaseEstimator, RegressorMixin, ValidateParams, EvaluateLoss):\n",
    "    \n",
    "    '''\n",
    "    Wrapper class for the three Beta regression models introduced in \n",
    "    Kull, M., Silva Filho, T.M. and Flach, P. Beta calibration: a \n",
    "    well-founded and easily implemented improvement on logistic \n",
    "    calibration for binary classifiers. AISTATS 2017.\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    This implementation is adapted from the `betacal` Python library \n",
    "    (https://github.com/dirmeier/betacal) with modifications.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator object, default=None\n",
    "        Base estimator used for calibration. It must be compatible \n",
    "        with the scikit-learn regressor interface (supporting parameter \n",
    "        management, fitting, prediction, and probability estimation).  \n",
    "        If None, defaults to LogisticRegression with parameters \n",
    "        `C=1e12`, and `fit_intercept=True`.\n",
    "        \n",
    "    parameters : {\"abm\", \"am\", \"ab\", \"sigmoid\"}, default=\"abm\"\n",
    "        Determines which parameters will be calculated by the model. \n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    calibrator_ : sklearn.linear_model.LogisticRegression\n",
    "        Internal calibration model corresponding to the chosen \n",
    "        parameterization.\n",
    "\n",
    "    '''\n",
    "    def __init__(self, parameters=\"abm\", estimator=None):\n",
    "        \n",
    "        if estimator is None:\n",
    "            kwargs = dict(C=1e12, fit_intercept=True)\n",
    "            self.estimator = LogisticRegression(**kwargs)\n",
    "        else:\n",
    "            self.check_estimator(estimator)\n",
    "            self.estimator = estimator.set_params(**{\"fit_intercept\":True})\n",
    "        \n",
    "        params = {\"abm\": ABM_BetaCal(), \"am\": AM_BetaCal() , \n",
    "                  \"ab\": AB_BetaCal() , \"sigmoid\": Sigmoid_Cal()} \n",
    "        self.parameters = self.StrOptions(\"parameters\", parameters, params.keys())\n",
    "        self.calibrator_ = params[self.parameters]\n",
    "        self.calibrator_.set_params(**{\"estimator\":self.estimator})\n",
    "\n",
    "    def fit(self, y_proba, y_true, sample_weight=None):\n",
    "        \n",
    "        '''\n",
    "        Fit the model using y_proba, y_true as training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : array-like, shape (n_samples,)\n",
    "            Training data.\n",
    "\n",
    "        y_proba : array-like, shape (n_samples,)\n",
    "            Probability estimates.\n",
    "            \n",
    "        sample_weight : array-like, shape = (n_samples,), default=None \n",
    "            Sample weights. If None, then samples are equally weighted.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of self.\n",
    "            \n",
    "        '''\n",
    "        self.calibrator_.fit(y_proba, y_true, sample_weight)\n",
    "        return self\n",
    "\n",
    "    def predict(self, y_proba):\n",
    "        \n",
    "        '''\n",
    "        Predict new values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_proba : array-like, shape (n_samples,)\n",
    "            Probability estimates.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_calib : array, shape (n_samples,)\n",
    "            The predicted values.\n",
    "\n",
    "        '''\n",
    "        # Check if fit() was called\n",
    "        check_is_fitted(self, \"calibrator_\")\n",
    "        return self.calibrator_.predict(y_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "757ab735",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedLogistic(BaseEstimator, RegressorMixin, ValidateParams):\n",
    "    \n",
    "    '''\n",
    "    Custom implementation of Logistic Regression using scipy's \n",
    "    `minimize` optimizer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    penalty : {\"l1\", \"l2\", \"elasticnet\", None}, default=\"l2\"\n",
    "        Type of regularization applied to the loss function.\n",
    "        Specify the norm of the penalty:\n",
    "    \n",
    "    tol : float, default=1e-4\n",
    "        Tolerance for stopping criteria.\n",
    "\n",
    "    C : float, default=1.0\n",
    "        Inverse of regularization strength; must be a positive float. \n",
    "        Like in support vector machines, smaller values specify \n",
    "        stronger regularization.\n",
    "\n",
    "    fit_intercept : bool, default=True\n",
    "        Whether to include an intercept term in the model.\n",
    "\n",
    "    class_weight : dict or \"balanced\" or None, default=None\n",
    "        Weights associated with classes in the form {label: weight}. \n",
    "        If not given, all classes are supposed to have weight one. \n",
    "        \n",
    "    solver : str, default=\"BFGS\"\n",
    "        Algorithm to use in the optimization problem from scipy's \n",
    "        `minimize`, which are {\"BFGS\", \"L-BFGS-B\", \"Nelder-Mead\", \n",
    "        \"Newton-CG\", \"CG\"}.\n",
    "     \n",
    "    max_iter : int, default=1000\n",
    "        Maximum number of iterations for the optimization solver.\n",
    "\n",
    "    l1_ratio : float, default=0.5\n",
    "        ElasticNet mixing parameter, with `0 <= l1_ratio <= 1`.\n",
    "        - l1_ratio = 1 → L1 penalty\n",
    "        - l1_ratio = 0 → L2 penalty\n",
    "        - 0 < l1_ratio < 1 → combination of L1 and L2 penalties\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    res_ : OptimizeResult\n",
    "        Full optimization result returned by `scipy.optimize.minimize`.\n",
    "        Contains diagnostic information such as final parameters,\n",
    "        convergence status, number of iterations, and gradient.\n",
    "\n",
    "    coef_ : ndarray of shape (n_features,)\n",
    "        Learned coefficients (weights) of the logistic regression model.\n",
    "\n",
    "    intercept_ : ndarray of shape (1,)\n",
    "        Intercept (bias) term added to the decision function.\n",
    "\n",
    "    loss_ : list of float\n",
    "        Sequence of loss values recorded at each optimization step.\n",
    "\n",
    "    params_ : list of ndarray\n",
    "        Sequence of parameter vectors (weights + intercept) evaluated\n",
    "        during optimization.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    methods = ['l1', 'l2', 'elasticnet', None]\n",
    "    solvers = [\"BFGS\", \"L-BFGS-B\", \"Nelder-Mead\", \"Newton-CG\", \"CG\"]\n",
    "    \n",
    "    def __init__(self, penalty=\"l2\", tol=1e-4, C=1e4, fit_intercept=True, \n",
    "                 class_weight=None, solver=\"BFGS\", max_iter=1000,  \n",
    "                 l1_ratio=0.5):\n",
    "        \n",
    "        # Validate all parameters\n",
    "        self.penalty = self.StrOptions('penalty', penalty, self.methods, str)\n",
    "        self.tol = self.Interval(\"tol\", tol, float, 0., 1., \"both\")\n",
    "        self.C = self.Interval(\"C\", C, float, 0., None, \"left\")\n",
    "        self.fit_intercept = self.StrOptions('fit_intercept', fit_intercept, [True, False], bool)\n",
    "        self.class_weight = self.check_class_weight(class_weight)\n",
    "        self.solver = self.StrOptions('solver', solver, self.solvers, str)\n",
    "        self.max_iter = self.Interval(\"max_iter\", max_iter, int, 1, None, \"left\")\n",
    "        self.l1_ratio = self.Interval(\"l1_ratio\", l1_ratio, float, 0., 1., \"both\")\n",
    "        self.xy_kwargs = dict(accept_sparse=True, dtype=float, \n",
    "                              ensure_2d=True, y_numeric=True)\n",
    "        \n",
    "        # Attributes\n",
    "        self.loss_ = list()\n",
    "        self.params_ = list()\n",
    "        \n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \n",
    "        '''\n",
    "        Fit the model according to the given training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            Training vector, where `n_samples` is the number of samples \n",
    "            and `n_features` is the number of features.\n",
    "\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Target vector relative to X.\n",
    "\n",
    "        sample_weight : array-like of shape (n_samples,) default=None\n",
    "            Array of weights that are assigned to individual samples.\n",
    "            If not provided, then each sample is given unit weight.\n",
    "            \n",
    "        References\n",
    "        ----------\n",
    "        [1] https://docs.scipy.org/doc/scipy/reference/generated/scipy.\n",
    "            optimize.minimize.html\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "            Fitted estimator.\n",
    "        \n",
    "        '''\n",
    "        # Validate inputs\n",
    "        X, y = check_X_y(X, y, **self.xy_kwargs)\n",
    "        self.n_features_ = X.shape[1]\n",
    "        if type_of_target(y) != \"binary\":\n",
    "            raise ValueError(\"This estimator only supports binary targets.\")\n",
    "        \n",
    "        # Initialize sample weights\n",
    "        sample_weight = _check_sample_weight(sample_weight, X)\n",
    "\n",
    "        # Default parameters\n",
    "        params = np.zeros(self.n_features_ + self.fit_intercept)\n",
    "        \n",
    "        # Apply class_weight if provided\n",
    "        if isinstance(self.class_weight, str):\n",
    "            classes = np.unique(y)\n",
    "            weights = compute_class_weight(self.class_weight, \n",
    "                                           classes=classes, y=y)\n",
    "            weights = dict(zip(classes, weights))\n",
    "        elif isinstance(self.class_weight, dict):\n",
    "            weights = self.class_weight\n",
    "        else: weights = None\n",
    "            \n",
    "        # `class_weight` x `sample_weight`\n",
    "        if isinstance(weights, dict):\n",
    "            sample_weight *= np.where(y==0, weights[0], weights[1])\n",
    " \n",
    "        # Optimizer\n",
    "        res = minimize(fun=lambda params: self.__optimize__(params, X, y, sample_weight)[0],\n",
    "                       x0=np.zeros(len(params)),\n",
    "                       jac=lambda params: self.__optimize__(params, X, y, sample_weight)[1],\n",
    "                       method=self.solver, \n",
    "                       tol=self.tol,\n",
    "                       options={\"maxiter\":self.max_iter})\n",
    "        \n",
    "        self.coef_ = res.x[:-1]\n",
    "        self.intercept_ = np.r_[res.x[-1]]\n",
    "        self.n_iter_ = res.nit\n",
    "        self.res_ = res\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def __optimize__(self, params, X, y, sample_weight):\n",
    "        \n",
    "        '''\n",
    "        Compute loss and gradient for optimization.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : ndarray of shape (n_features + 1,)\n",
    "            Flattened array of weights and bias.\n",
    "\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Training data.\n",
    "\n",
    "        y : ndarray of shape (n_samples,)\n",
    "            True labels.\n",
    "            \n",
    "        sample_weight : array-like of shape (n_samples,) \n",
    "            Array of weights that are assigned to individual samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : float\n",
    "            Logistic loss with regularization.\n",
    "\n",
    "        gradients : ndarray of shape (n_features + 1,)\n",
    "            Gradient of the loss with respect to weights and bias.\n",
    "        \n",
    "        '''\n",
    "        if self.fit_intercept:\n",
    "            w = params[:-1]\n",
    "            b = params[-1]\n",
    "        else:\n",
    "            w = params\n",
    "            b = 0.0\n",
    "\n",
    "        # Compute score\n",
    "        z = X @ w + b\n",
    "        y_pred = 1 / (1 + np.exp(-z))\n",
    "        \n",
    "        # Loss with penalty\n",
    "        loss = log_loss(y, y_pred, sample_weight=sample_weight) + self.__penalty__(w)\n",
    "        self.loss_.append(loss)\n",
    "\n",
    "        # Compute gradients\n",
    "        error = (y_pred - y) * sample_weight\n",
    "        dw = (X.T @ error) / np.sum(sample_weight) + self.__regularization__(w)\n",
    "        db = np.sum(error) / np.sum(sample_weight) if self.fit_intercept else 0.\n",
    "        \n",
    "        gradients = np.r_[dw, db] if self.fit_intercept else np.r_[dw]\n",
    "        self.params_.append(params)\n",
    "        \n",
    "        return loss, gradients\n",
    "    \n",
    "    def __penalty__(self, w):\n",
    "        \n",
    "        '''\n",
    "        Compute the penalty term for the loss function. The degree of\n",
    "        penalties depends on weights, regularization strength, and \n",
    "        `l1_ratio` (only relevant when \"elasticnet\" is selected)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        w : ndarray of shape (n_features,)\n",
    "            Model weights.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        penalty : float\n",
    "            Penalty value added to the loss.\n",
    "            \n",
    "        '''\n",
    "        l1 = 1 / self.C * np.sum(np.abs(w))\n",
    "        l2 = 0.5 * 1 / self.C * np.sum(w**2)\n",
    "        \n",
    "        if self.penalty is None:\n",
    "            return 0\n",
    "        elif self.penalty == \"l1\":\n",
    "            return l1 \n",
    "        elif self.penalty == \"l2\":\n",
    "            return l2\n",
    "        elif self.penalty == \"elasticnet\":\n",
    "            return self.l1_ratio * l1 + (1 - self.l1_ratio) * l2\n",
    "    \n",
    "    def __regularization__(self, w):\n",
    "        \n",
    "        '''\n",
    "        Compute the regularization term for the gradient. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        w : ndarray of shape (n_features,)\n",
    "            Model weights.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        grad_penalty : ndarray of shape (n_features,)\n",
    "            The array of penalties with respect to `w`. \n",
    "            \n",
    "        '''\n",
    "        l1 = 1 / self.C * np.sign(w)\n",
    "        l2 = 1 / self.C * w\n",
    "        \n",
    "        if self.penalty is None: \n",
    "            return 0\n",
    "        elif self.penalty == \"l1\":\n",
    "            return l1\n",
    "        elif self.penalty == \"l2\":\n",
    "            return l2\n",
    "        elif self.penalty == \"elasticnet\":\n",
    "            return self.l1_ratio * l1 + (1 - self.l1_ratio) * l2\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        '''\n",
    "        Predict class probabilities for input samples.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_proba : ndarray of shape (n_samples, 2)\n",
    "            Probability estimates for the negative and positive class.\n",
    "        \n",
    "        '''\n",
    "        X = check_X_y(X, np.ones(len(X)), **self.xy_kwargs)[0]\n",
    "        z = X @ self.coef_ + self.intercept_\n",
    "        y_proba = 1 / (1 + np.exp(-z))\n",
    "        return np.vstack([1 - y_proba, y_proba]).T\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \n",
    "        '''\n",
    "        Predict binary labels for input samples.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Input samples.\n",
    "\n",
    "        threshold : float, default=0.5\n",
    "            Threshold used to assign class labels.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : ndarray of shape (n_samples,)\n",
    "            Predicted class labels (0 or 1).\n",
    "            \n",
    "        '''\n",
    "        y_proba = self.predict_proba(X)\n",
    "        return np.where(y_proba >= threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a44e5f-e667-4692-b241-f21df8bf0593",
   "metadata": {},
   "source": [
    "### <font color=\"green\" size=5> Make classification data </font>\n",
    "\n",
    "Generate a random n-class classification problem. [**`make_classification`**](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6ef5dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, sys\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "from sklearn.ensemble import RandomForestClassifier as rf\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2d99d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples     = 10000, \n",
    "                           n_features    = 20, \n",
    "                           n_informative = 10, \n",
    "                           n_redundant   = 10, \n",
    "                           random_state  = 0, \n",
    "                           shuffle       = True, \n",
    "                           n_classes     = 2, \n",
    "                           weights       = [0.7, 0.3])\n",
    "\n",
    "X = pd.DataFrame(X, columns=[\"feature_\" + str(n).zfill(2) \n",
    "                             for n in range(1,X.shape[1]+1)]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1163b88-abd1-4cad-a4ea-5a1642087949",
   "metadata": {},
   "source": [
    "Split **X** and **y** into random train and test subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9161da0f-ada0-46fd-a68b-258c9c103d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tts_kwds = {\"test_size\"   : 0.3,\n",
    "            \"random_state\": 0, \n",
    "            \"shuffle\"     : True}\n",
    "X_train, X_test, y_train, y_test = tts(X, y, **tts_kwds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe8ba4c-3b94-4d3e-b1d8-336c84e26d3e",
   "metadata": {},
   "source": [
    "Train model with **train** set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2499e33e-f38e-4985-bb85-152fd075e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = rf(**{\"random_state\": 0, \n",
    "            \"n_estimators\": 200, \n",
    "            \"n_jobs\"      : -1, \n",
    "            \"class_weight\": \"balanced\", \n",
    "            \"max_depth\"   : 7, \n",
    "            \"max_features\": \"sqrt\"})\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_train_proba = clf.predict_proba(X_train)[:,1]\n",
    "y_test_proba  = clf.predict_proba(X_test )[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2285260-ecdb-4002-ad4e-ed7148d43bdc",
   "metadata": {},
   "source": [
    "### <font color=\"green\" size=5> Calibration of probability </font>\n",
    "\n",
    "The purpose of [`calibration`](https://scikit-learn.org/stable/modules/calibration.html)  is to improve estimated probabilities, which can be directly interpreted as a confidence level. For instance a well calibrated (binary) classifier should classify the samples such that among the samples to which it gave a **`predict_proba`** value close to 0.8, approx. 80% actually belong to the positive class. The samples that are used to fit the calibrator should not be the same samples used to fit the classifier, as this would introduce bias. Using logistic regression as a calibrator, we are trying to find the optimal set of coefficients that yields the smallest difference of given evaluation metrics e.g. [**`log_loss`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html). \n",
    "\n",
    "Please be noted that the following example does not conclude the optimal way of calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f562817-6717-4033-b9e6-78de0f95842f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2008b485-e633-4c3e-ae44-b21790b947b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b00bff1-cb15-45ad-b79e-5f0f49b35478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5317f0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters(a=0.042975873186836815, b=0.005138928638810374, c=0.0, m=0.5)\n",
      "Losses(brier_score=[0.03543610488671236, 0.017233992475398342], log_loss=[0.13450623678484486, 0.0871611620571625])\n"
     ]
    }
   ],
   "source": [
    "a = BetaCalibration(\"ab\", LogisticRegression(C=0.001, fit_intercept=False))\n",
    "a.fit(probs_cal, y_cal)\n",
    "# print(a.calibrator_.estimator.get_params())\n",
    "print(a.calibrator_.params_)\n",
    "print(a.calibrator_.losses_)\n",
    "X = a.calibrator_.__transform__(probs_cal.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f5637d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
       "  success: True\n",
       "   status: 0\n",
       "      fun: 0.6008582065808956\n",
       "        x: [ 3.797e-01  1.369e+00]\n",
       "      nit: 9\n",
       "      jac: [-8.046e-04  4.493e-04]\n",
       "     nfev: 11\n",
       "     njev: 11\n",
       " hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ModifiedLogistic(penalty=\"elasticnet\", tol=1e-4, C=1e12, fit_intercept=True, \n",
    "                 class_weight=\"balanced\", solver=\"L-BFGS-B\", max_iter=100,  \n",
    "                 l1_ratio=0.6)\n",
    "model.fit(X, y_cal)\n",
    "model.res_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4735b468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters(a=0.4419318904945563, b=0.4419318904945563, c=-6.269642963778159, m=0.999999310224319)\n",
      "Losses(brier_score=[0.03543610488671236, 0.01755714706954703], log_loss=[0.13450623678484486, 0.1304699905682189])\n"
     ]
    }
   ],
   "source": [
    "a = BetaCalibration(\"am\", ModifiedLogistic(penalty=\"elasticnet\", \n",
    "                                           tol=1e-4, C=1e12, \n",
    "                                           fit_intercept=True, \n",
    "                                           class_weight={0:100,1:2}, \n",
    "                                           solver=\"L-BFGS-B\", \n",
    "                                           max_iter=100,  \n",
    "                                           l1_ratio=0.6))\n",
    "a.fit(probs_cal, y_cal)\n",
    "# print(a.calibrator_.estimator.get_params())\n",
    "print(a.calibrator_.params_)\n",
    "print(a.calibrator_.losses_)\n",
    "X = a.calibrator_.__transform__(probs_cal.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "939aa590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x24da31d6160>]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.array(model.params_)[2:,0])\n",
    "plt.plot(np.array(model.params_)[2:,1])\n",
    "# plt.plot(np.array(model.params_)[2:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "429af199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x24da332fa30>]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAAsTAAALEwEAmpwYAAAaH0lEQVR4nO3dC3Rc1X3v8d9fb+ttPfySZcsvHjYYKMKEAA4Nl1faQtsLubh5kVBgtZd70+Teu0rTVZpFVlebtmly27JSIIECTUJo0tzQ1BfnAYSEEGLZgI3tGmTjl/BDtmTZlizrtbv28TGdyJI9I81oZs75ftY6mZkz58j7MNFvjvbe53/MOScAQHQVZLsBAIDMIugBIOIIegCIOIIeACKOoAeAiCtSjmloaHAtLS3ZbgYA5JV169YddM415kXQ+5Bva2vLdjMAIK+Y2c7x3qPrBgAijqAHgIgj6AEg4gh6AIg4gh4AIo6gB4CII+gBIOJybh79RPUNDOkfXtiW8n5Xn9Ooy1rqMtImAMgFkQn64wPD+rvn21Pax5fi/9vn2vUbF83RZz5wnmbXTMtY+wAgWyIT9PWVpXr7z38t5S+HL/94m/7hx9v0w837de/7F+t3r16g0qLCjLUTAKZarPvop5UU6tPXnaMfffp9WnlOg/5qzVZd/8UX9aMt+7PdNABIm1gH/SnNdeV66COtevLOFSoqMN35eJs+/tgv9PbB3mw3DQAmjaBPcPWSRv3/T67UH3/gfK3d0a0bvviiPv/sv6v3xFC2mwYAE0bQj1JSVKC7Vi7Uc//7fcEg7Zdf2KZrv/Bjffe1DnEjdQD5yHItvFpbW10ulSlet7Nbn31mkzZ29GhFS50+9J55KipI/vtxfn25LmiqyWgbAcDM1jnnWsd8j6A/u+ERp6fbdgeDtV29AyntW1pUoJfue78aKksz1j4AsDMEfWSmV2ZSYYFp1Yp5uvmiOeo4fDzp/Q4cOaEPf/UVPfHyzmB2DwBkA0GfgorSIp0zsyrp7f22/+X8mXry5R36vfctCqZzAsBUYzA2w+5euVDdfYP69vo92W4KgJgi6DPsspbpuqi5Vl/96dtBXz8ATDWCPsPMTHddvSC4+OqHXHELIAsI+ilw47JZmjt9mh55cXu2mwIghgj6KVBUWKA7r1qgtp3dwbx8AJhKBP0U+WBrs6rLivSVn3BWD2BqEfRTODXzw++Zr2c37dPOQxRLAzB1CPopdMd7W4LqmH4GDgBMFYJ+Cs2oLtNvXtwUlFPoTrGUAgBMFEE/xX736oXqHxzR117Zme2mAIgJgn6KnTurSu87p1H/+LOd6h8cznZzAMRAUkFvZjea2VYzazez+8Z4f6WZrTezITO7NWH9xWb2spltMrMNZvbf0n0A+VoW4eCxE0GNewDIetCbma/E9aCkmyQtlbTKzPxjol1+rFHS10et75P0UefcMn/dkKQvmVmtYu69i+q1dHa1HvnJ2xqhLAKAHDijXyGp3Tm33TnnRxCfknRL4gbOuR3OuQ2SRkatf9M591b4/B1fuVdSo2LOl0XwZ/XtB47phTf9fxIAyG7QN0nanfB6T7guJWbmvzBKJG1Ldd8o+rXlszW7pkwPUxYBQBQGY81stqQnJX3cOTcyxvt3m1mbXzo7OxUHxYUF+sSVC/Tz7V3asOdwtpsDIOZB70cMmxNezw3XJcXMqiX9m6Q/ds79fKxtnHMP+1tg+aWxMT49O7evaFZVaVHQVw8A2Qz6tZKWmNkCM/NdL7dLeiaZHx5u/x1JTzjnvjX55kZLVVlxEParN+7Vnm4/bg0AWQh659yQpHslrZG0RdLTzjk/XfIBM7vZb2Nml5mZ77u/TdJDfjpluPsHJa30M3LM7LVwuTgDx5G3Pn7lApmkx17ake2mAIgocy63pve1tra6trY2xckfPPWqfrB5v372R9eqZlpxtpsDIA+Z2Trf/T3We1wZmyNlEXoHhvWNX/jLEQAgvQj6HHBBU42uXFyvx156WwNDp01KAoBJIehzxF1XL9T+Iyf0r6/768oAIH2K0vizMAm+0Nm5M6v04Avt6jk+mPR+ZcWF+q+XNqm0yFeqAIDTEfQ5VBbh9391kT751Gt64HubU9q3qqxIv3HRnIy1DUB+I+hzyC0XN+na82dqeDi5mVBDIyN67188p9d3HyboAYyLoM8xlaWpfSRL51RrQ0dPxtoDIP8xGJvnljfV6I2OHg1T7hjAOAj6PLd8bq36Boa1rfNYtpsCIEcR9Hlu+dya4HHDHrpvAIyNoM9zCxsrVVFSSKljAOMi6PNcYYFpWVMNZ/QAxkXQR8BFc2u0ee8RyicAGBNBH5EBWR/yb+4/mu2mAMhBBH0EMCAL4EwI+giYV1ce1LFnQBbAWAj6iNTJ8Wf1nNEDGAtBHxE+6LfuP6r+weFsNwVAjiHoIzQg68sg+Nk3AJCIoI/agOxu+ukB/DKCPiJmVZepsaqUSpYATkPQR2lAlitkAYyBoI9YP72vYnnsxFC2mwIghxD0Eeund05BfXoAOIWgj+QVsgzIAvhPBH2E1FeWqql2Gv30AH4JQR8xXCELYDSCPoIDsru6+tTdO5DtpgDIEQR9RPvpNzIgCyBE0EfMBU0MyAL4ZQR9xPhyxQsaKuinB/Augj6CGJAFkIigj+iA7L4j/TpwpD/bTQGQAwj6COLWggASEfQRtGxOtQqMAVkAJxH0EVReUqQlM6ooWQwgQNBHfEDW+SpnAGItqaA3sxvNbKuZtZvZfWO8v9LM1pvZkJndOuq9Z83ssJl9L50Nx9mDvqt3QB2Hj2e7KQByPejNrFDSg5JukrRU0ioz84+Jdkm6Q9LXx/gRfyXpI+lrMpKdeeMxIAsgmTP6FZLanXPbnXO+gMpTkm5J3MA5t8M5t0HSyOidnXM/knQ0ra3GWZ03u0rFhUbQA0gq6Jsk7U54vSdclzZmdreZtfmls7MznT86tkqLCnXerGpm3gDIjcFY59zDzrlWvzQ2Nma7OZHqp9+4p0cjIwzIAnGWTNB3SGpOeD03XIc8CPqjJ4a041BvtpsCIMeDfq2kJWa2wMxKJN0u6ZkpaBsmiQFZAEkFvXNuSNK9ktZI2iLpaefcJjN7wMxu9tuY2WVm5vvub5P0kJltOrW/mf1E0j9LutZvY2Y38J9+aiyZUamy4gKCHoi5omQ2cs6tlrR61Lr7E56vDbt0xtr36jS0ExNQVFigZXP8hVMMyAJxlhODschsP/2md45oaPi0ma8AYoKgj0HQHx8cVnvnsWw3BUCWEPRxGZDdTT89EFcEfcQtqK9QVWmRNnTQTw/EFUEfcQUFFtwwnJk3QHwR9DGwvLlGW/Ye0Ymh4Ww3BUAWEPQxsLypVoPDTlv3UVsOiCOCPkb3kH2d7hsglgj6GJg7fZqmlxdrIxdOAbFE0MeAmQXTLBmQBeKJoI9R982b+4/q+AADskDcEPQx4c/ofVn6Te9wVg/EDUEfswFZum+A+CHoY2JmdZlmVpdSyRKIIYI+RhiQBeKJoI+R5U012n6wV0f6B7PdFAC5duMRRMPy5pOVLH+0Zb8ubDr5PBnz68tVXMg5AZCvCPqYndEXmPSpb76e0n4funye/uy3LsxYuwBkFkEfI9MrSvT0PVfonZ7+pPd56MfbtLGDfn0gnxH0MdPaUpfS9ut3duuf23b7e/8GV9gCyD90vOKMFs+oVO/AsPam8FcAgNxC0OOsQe+9dYB7zgL5iqBHUkHfTtADeYugxxnVV5SotryYoAfyGEGPM/IDsIsbK7WNoAfyFkGPpLpv2jsJeiBfEfRIKui7egeCBUD+IehxVgzIAvmNoEcKUyyPZrspACaAoMdZzamZpmnFhZzRA3mKoMdZFRSYFs2oIOiBPEXQIylMsQTyF0GPpPvpfdXL3hND2W4KgBQR9EjK4hlVweM25tMDeYegR1KYYgnkL4IeSd9OsKjAqGIJRDXozexGM9tqZu1mdt8Y7680s/VmNmRmt45672Nm9la4fCydjcfU8feMbWlg5g0QyaA3s0JJD0q6SdJSSavMzD8m2iXpDklfH7Wvv53Rn0q6XNIK/9zMpqf9KDAlmHkDRPeM3gd0u3Nuu3POFzt5StItiRs453Y45zZIGhm17w2SfuCc63LOdfvnkm5M7yFgKvvpd3b1aWBo9McMIN+DvknS7oTXe8J1yvC+yMGgHx5x2nGoN9tNAZBvg7FmdreZtfmls7Mz283BOJh5A0Q36DskNSe8nhuuU7r2dc497Jxr9UtjY2OSPxpTbVFjpcwIeiCKQb9W0hIzW2BmJZJul/RMkj9/jaTr/QBsOAh7fbgOeWhaSaGaaqcxxRKIWtA75/w17/eGAb1F0tPOuU1m9oCZ3ey3MbPLzMz3v98m6SEz2xTu2yXpc+GXhV8eCNchn+82RdADeaUomY2cc6slrR617v6E52vDbpmx9n1Ukl8QkSmWL287FAzKFhZYtpsDIF8GY5FfZ/QnhkbU0X08200BkCSCHhObedPJ3aaAfEHQIyVMsQTyD0GPlNSWl6ihspSgB/IIQY+ULZ5RwRRLII8Q9JjwFEvnXLabAiAJBD0mNMXyaP+QOo+eyHZTACSBoMeEbytIPz2QHwh6TGKKJUEP5AOCHimbWV2qqtIizuiBPEHQI2VmpkXUvAHyBkGPCXffMMUSyA8EPSYc9H7WTc/xwWw3BcBZEPSY8BRLj+4bIPcR9JjUzJttBD2Q8wh6TEhzXblKigqYYgnkAYIeE+JvOrKwoYKuGyAPEPSYMG4rCOQHgh6TCvrd3X3qHxzOdlMAnAFBj0kFvS9guY1+eiCnEfSYMO42BeQHgh4TtqChQgXGFEsg1xH0mLDSokLNqytniiWQ4wh6TLo2PV03QG4j6DHpfvq3D/ZqaHgk200BMA6CHpMO+sFhp51dfdluCoBxEPSYFGbeALmPoMekLGqsCB4JeiB3EfSYlKqyYs2qLmOKJZDDCHqkp+YNUyyBnEXQIy1B78/ona+HACDnEPRIS9D3Dgxrb09/tpsCYAwEPSaNmTdAbiPokbagf4ugB3ISQY9Jq68oUW15MWf0QI4i6DFpZqbFjScHZAHkHoIeacEUSyDPg97MbjSzrWbWbmb3jfF+qZl9M3z/FTNrCdeXmNljZrbRzF43s2sycRDIjaDv6h0IFgB5FvRmVijpQUk3SVoqaZWZ+cdEd0rqds4tlvRFSZ8P19/l/8c5d6Gk6yR9wcz4KyKCmHkD5K5kQneF//11zm13zvnTtack3TJqG//68fD5tyRda77j9uQXw3N+pXPugKTDklrTfxjINoIeyO+gb5K0O+H1nnDdmNs454Yk9fjJGJJel3SzmRWZ2QJJl0pqHv0PmNndZtbml87OzkkfFKbenJppmlZcqLcOHM12UwCMkululEfDL4Y2SV+S9DNJw6M3cs497Jxr9UtjY2OGm4RMKCgwLZpRwRk9kIOKktimY9RZ+Nxw3Vjb7PFn75JqJB1yJ4uffOrURmbmg/7N9DUfucRPsfzF213ZbgaACZzRr5W0xHe9+Fk0km6X9Myobfzrj4XPb/X98j7kzazczIKC5WbmB2OHnHObk/g3kaf99O/09Kv3hO+9A5A3Z/S+z93M7pW0RpKfgfOoc26TmT3gu2Sccz7kvyrpST+9UlJX+GXgzfD7mdlIeNb/kcwfErJ5o3BvW+cxLZ9bm+3mAEih68aH/WpJq0etuz/huS9beNsY++2QdG4y/waiNfOGoAdyB3PakTbz68tVVGAMyAL5eEYPJKO4sEAtDRX6aftBNf9iV9L7zasr15WLGzLaNiDOCHqk1a/Mq9XTbXu0Yc/GlPb7k19fqjuv8pdaAEg3gh5p9ee/vVyfvi75YRknpwf+dbM+973NKi40ffSKoEwSgDQi6JFWhQWmWTVlKe3zf2+/RINfW6/7v7tJRQUF+p3L52WsfUAcMRiLrCspKtCDH7pEv3puoz7znY16em1ixQ0Ak0XQIyeUFhXqyx++VFcvadAf/ssGfXudr5wBIB0IeuSMsuJCPfLRVr13Ub3+z7de13dfG11pA8BEEPTIubD/ykcv02UtdfrUN1/Tv23Ym+0mAXmPoEfOmVZSqEfvuEyXzp+u//nUq3r2jX3ZbhKQ1wh65KSK0iI99vEVWj63Rv/jG+v1w837s90kIG8R9MhZlaVFevwTK7R0drV+/2vr9fxWf5MyAKki6JHTqsuK9cQnLtc5syp1z5Pr9OKb3IEMSBVBj5xXU16sJz9xuRY1VuquJ9r0UvvBbDcJyCtcGYu8ML2iRP905wqteuTn+tBXXgmqZKZym8MVLXW6YdlMXb9slmZWp3blLpDv7OTd/nJHa2ura2vzt5gFTnfw2Al945Vd6h867dbD4+o9MRx0+Ww/2Bu8vmRerW5cNks3LJsVVNsEosDM1vn7bo/5HkGPOPD/P/d18v1UzTWb9+mNjiPB+nNnVumGC3zozwwGfc2S/0sByCUEPTDK7q4+fX/zfq3ZtE9rd3TJ/xo0103TDUtnBcE/d/q0lH7e9PKS4GIvIFsIeuAs3UE/DEP/pfZDGhj2tzhOTUlhgS5oqlZrS11woZdfGipLM9JeYCwEPZCko/2D+ulbB9VzfDDpffxv0I6DvWrb2a2Ne3re/aJoqS/XpfNPBn9ry3QtbqwMBoaBqQ56Zt0ACarKinXThbMnvP+JoWG90dGjth3dQfC/sPWAvr3+ZCXOmmnFwR24fPD7qaKpjAdUlRUFXxa+yieQKoIeSCMfxCfP4ut0TzgIvONQn9p2dGn9ru7gC+D5rRO76KuipFDXnDcjmC10zbmNwcVkQDIIeiCD/Fn7goaKYLmttTlYd7hvQO8c7k/p5+w7clw/2Lw/WHxFT3/bxSsWNQSzha47f6ZmcG0AzoA+eiCPDI84vbqr+90ZQzsP9cn3AF3SXBtcDObP9v2XCuLHGIwFosf/7r65/1gQ+N9PuDZgyYzKoGunsjT5rh0/Rnz+7GqtWFhHl1CeIuiBGNjT3Rd07Zy8NqA7OPtPlQ/8C5pqdMXCer1nUX1wAxhfRRS5j6AHYmYkxZD3U0Jf3XVYL28/pJ9vO6RXd3drcNipsMCCewL44L9iUb1a59cFN4ZB7iHoAaTk+MCw1u3s1svbD+rlbYe0YU+PhkZcMAh8cXNtEPwza1IbAPZdQvPqyjW/vly15SUZa3tcGfPoAaTCn7VftaQhWLzeE0NBqYhTZ/x//3y7JtAz9K7qsiLNr6/QvPryk+FfVx489+tmVZcFf0kgfQh6AEnd2vGac2cEy6ng90uy/HdCd9+Adh3q066uvmC20M6uPm3q6NGaN/YFfy0klpPwtYYaq0qDGUXJ8l8OtdNKVFteHNQeSnysLS/R9PC1v3AtblcoE/QAJhT8fkmFvw/AebOqT1s/NDyivT39CV8AvUHRuYPHBoJic8kaGBjW3p4jOtw3GFyrMN5fHGYnr1KuTUPgj9579NXOqf7082ZX6+9WXaJ0I+gBZFVRYYGa68qD5crF6RuMPto/FPwV0d03oMPHT4Z/d2/42DcY1DMamcQYpTvLCnf6FmfVnGLV1GQR9AAix5+p+1tQ+qVFXEDGPWMBIOIIegCIOIIeACIuqaA3sxvNbKuZtZvZfWO8X2pm3wzff8XMWsL1xWb2uJltNLMtZvZHmTgIAMAkgt7M/PXOD0q6SdJSSavMzD8mutNPk3XO+THzL0r6fLj+Nl+i2zl3oaRLJd1z6ksAAJA7Z/QrJLU757Y75wYkPSXpllHb+NePh8+/JelaOzmh1M8vqjAzP7vHzxvy+58ssQcAyJmgb5K0O+H1nnDdmNs45/zlcj2S6sPQ75W0V9IuSX/tnOtK7yEAALI5GOv/GhiWNEfSAkn/y8wWjt7IzO42sza/dHZO7DZrAICJXzDV4S/YSng9N1w31jZ7wm6aGkmHJP2OpGedc4OSDpjZS75ApaTtiTs75x6W9HAY+p1mtlMT56swHVS8xO2Y43a8HsccDw2TOOb5kwn6tf6mNWa2IAz028MAT/SMpI9JelnSrZKec845M/PdNe+X9KSZ+cvT3iPpS2f6x5xzjZoE/1fBeKU6oypuxxy34/U45niwDB3zWbtuwj73eyWtkbRF0tPOuU1m9oCZ3Rxu9lXfJ++nV0r6tKRTUzD9bJ1KM9sUfmE85pzbkO6DAABMstaNc261pNWj1t2f8Lw/nEo5er9jY60HAEydKF4ZG/T1x0zcjjlux+txzPHwcCZ+aM7dShAAkF5RPKMHACQg6AEg4iIT9GcrvBZFZrYjLBj3mp+WpQgys0fNzF+D8UbCujoz+4GZvRU+Tlf0j/mzZtYRftZ++YAixMyazex5M9vsZ+mZ2Sej/Fnb+Mebkc85En30YeG1NyVdF5Zo8FM5VznnNiviQe8vQHPORfaiEjNbKcnP3nrCOXdBuO4vJXU55/4i/FKf7pz7Q0X7mD/r1znn/loRZGazJc12zq03sypJ6yT9pqQ7ovhZ2/jH+8FMfM5ROaNPpvAa8pBz7kX/i36GInqPh78gUT/mSHPO7fWhFz4/Gl6z0xTVz9qNf7wZEZWgT6bwWhT5P8e+b2brfL0gxcdM/4sSPt/nXyse7jWzDWHXTiS6MMYSljK/RNIrcfis7ZePNyOfc1SCPq6ucs79SnivgP8e/skfK77URviFF3VflrRI0sVhNdgvKILMrFLStyX9gXPuSNQ/azv9eDPyOUcl6JMpvBY5zrngGJ1zByR9J+zCioP9YR/nqb5Of/yR5pzb75wbds6NSHokip+1vyNdGHpfc879S9Q/axvjeDP1OUcl6N8tvGZmJWHhNV9oLbJ8kbhwECd4Lul6Se/O0oi4U0X0FD5+VxF3KuxCvxW1zzq8UZGvmbXFOfc3Uf+sbZzjzdTnHIlZN144DclXxvQzcB51zv2ZIiys6+/P4k/VLPp6FI/ZzL4h6ZqwfOt+SX8q6f/54nqS5knyJa0/GKUb2oxzzNeEf877X1g/2+qehL7rvGdmV0n6iaSNkvzZrPeZsN86cp+1jX+8qzLxOUcm6AEA0e66AQCMg6AHgIgj6AEg4gh6AIg4gh4AIo6gB4CII+gBQNH2H9+85fr+PgTKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.loss_[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764d0494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae20112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd33a72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
